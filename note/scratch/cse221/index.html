<!DOCTYPE html>
<html lang="en">
<head>
<title>Notes of CSE 221: Operating Systems</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="./../../note-style.css">
<link rel="icon" type="image/png" href="./../../../favicon.png">
</head>
<body>
<header id="header" class="obviously-a-link">
<nav><a href="../../../index.html">Home</a><span>▶︎</span><a href=
"../../index.html">Notes</a><span>▶︎</span></nav>
</header>
<main id="body">
<article>
<div class="title">
<h1 class="main-title">Notes of CSE 221: Operating Systems</h1>
<p class="subtitle">UCSD recap, ep1</p>
</div>
<nav id="toc" class="obviously-a-link">
<h2>Table of Contents</h2>
<ol>
<li><a href="#THE%20System">THE System</a></li>
<li><a href="#Nucleus">Nucleus</a></li>
<li><a href="#HYDRA">HYDRA</a></li>
<li><a href="#TENEX">TENEX</a></li>
<li><a href="#MULTICS">MULTICS</a></li>
<li><a href="#Protection">Protection</a></li>
<li><a href="#UNIX">UNIX</a></li>
<li><a href="#Plan%209">Plan 9</a></li>
<li><a href="#Medusa">Medusa</a></li>
<li><a href="#Pilot">Pilot</a></li>
<li><a href="#Monitor">Monitor</a></li>
<li><a href="#V%20Kernel">V Kernel</a></li>
<li><a href="#Sprite">Sprite</a></li>
<li><a href="#Grapevine">Grapevine</a></li>
<li><a href="#Global%20memory">Global memory</a></li>
<li><a href="#%CE%BC-kernel">μ-kernel</a></li>
<li><a href="#Exokernel">Exokernel</a></li>
<li><a href="#Xen">Xen</a></li>
<li><a href="#VMS">VMS</a></li>
<li><a href="#Mach">Mach</a></li>
<li><a href="#FFS">FFS</a></li>
<li><a href="#LFS">LFS</a></li>
</ol>
</nav>
<p>During my time at <span class="smallcaps">ucsd</span>, I took all the
main courses in their systems track and enjoyed them greatly. It would be
a shame if I forgot those wonderful things I learn, due to my
extraordinarily bad memory. So here is a hodgepodge of lecture notes,
personal notes, and faint recollections.</p>
<p>Just be aware, I wrote this to help myself rather than introducing
stuff to others, so this article might not be very interesting or useful
for others. Neither do I guarantee the correctness of any information in
this article.</p>
<p><span class="smallcaps">cse</span> <span class=
"oldstyle-num">221</span> is the entry course, introducing students to
reading papers and the essential systems papers. The cast of papers is
pretty stable over the years and across instructors.</p>
<p>Here is a syllabus of a <span class="oldstyle-num"><span class=
"smallcaps">cse 221</span></span> similar to the one I took: <a href=
"https://cseweb.ucsd.edu/classes/wi21/cse221-a/readings.html"><em>CSE
221: Reading List and Schedule, Winter 2021</em></a>. (I took the one in
Fall <span class="oldstyle-num">2021</span> taught by Dr. Y. Y.
Zhou.)</p>
<h2 id="THE%20System" class="section">THE System</h2>
<p><em>The Structure of the “THE”-Multiprogramming System</em>,
<span class="oldstyle-num">1968</span>, by none other than Edsger W.
Dijkstra.</p>
<p>The main take away is “central abstraction in a hierarchy”. The
central abstraction is sequential process, and hierarchy is basically
“layers”. The benefit of layers is that it’s easy to verify soundness and
prove correctness for each individual layer, which is essential to handle
complexity. To quote Dijkstra:</p>
<blockquote>In testing a general purpose object (be it a piece of
hardware, a program, a machine, or a system), one cannot subject it to
all possible cases: for a computer this would imply that on feeds it with
all possible programs! Therefore one must test it with a set of relevant
test cases. What is, or is not, relevant cannot be decided as long as one
regards the mechanism as a black box; in other words, the decision has to
be based upon the internal structure of the mechanism to to be tested. It
seems to be the designer’s responsibility to construct his mechanism in
such a way—i.e. so effectively structured—that at each state of the
testing procedure the number of relevant test cases will be so small that
he can try them all and that what is being tested will be so perspicuous
that the will not have overlooked any situation.</blockquote>
<p>He then mentioned that “industrial software makers” has mixed feelings
of this methodology: they agree that it’s the right thing to do, but
doubt whether it’s applicable in the real world, away from the shelter of
academia. Dijkstra’s stance is that the larger the project, the more
essential the structuring. This stance is apparent in <a id=
"footref:ewd1041" class="footref-anchor obviously-a-link" aria-label=
"Jump to footnote" href="#footdef%3Aewd1041">his other
writings<sup class="inline-footref">1</sup></a>.</p>
<div id="footdef:ewd1041" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Aewd1041">1</a></div>
<div class="def-footdef"><a href=
"https://www.cs.utexas.edu/users/EWD/transcriptions/EWD10xx/EWD1041.html">
<em>EWD 1041</em></a>. Now, I don’t think it is realistic to write proofs
for every system you design, but good structuring, and designing with
testing in mind are certainly essential.</div>
</div>
<p>The downside of layering is, of course, the potential loss of
efficiently, due to either the overhead added by layering, or the lack of
detail hidden by lower layers. For example, the graphic subsystem in
win<span class="oldstyle-num">32</span> was moved into the kernel in
<span class="oldstyle-num"><span class="smallcaps">nt4</span></span>,
because there were too many boundary crossing.</p>
<p>And sometimes it’s hard to separate the system into layers at all, eg,
due to circular dependency, etc. For example, in Linux, memory used by
the scheduler is pinned and never page.</p>
<p>Professor also noted some interesting terminology used at the time,
like “harmonious cooperation”—no deadlock, and “deadly embrace”—deadlock.
Also, do you know why the function name for wait and signal for
semaphores are P and V? Because they are in Dutch, P stands for proberen,
and V stands for verhogen.</p>
<h2 id="Nucleus" class="section">Nucleus</h2>
<p><em>The Nucleus of a Multiprogramming System</em>, <span class=
"oldstyle-num">1970</span>.</p>
<p>Basically they want a “nucleus” (small kernal) that supports multiple
simultaneous operating system implementations. So the user can have their
<span class="smallcaps">os</span> however they want. (Another example of
“mechanism instead of policy”, sort of.) This school of thought would
later lead to exokernel and micro kernel.</p>
<p>The nucleus provides a scheduler (for process and <span class=
"smallcaps">i/o</span>), communication (messages passing), and primitive
for controlling processes (create, start, stop, remove).</p>
<p>In their design, the parent process is basically the <span class=
"smallcaps">os</span> of their child processes, controlling allocation of
resources for them: start/stop them, allocate memory and storage to them,
etc. Although a parent process can start/stop a child process, it doesn’t
have control over the exact scheduling: the nucleus is in charge of that,
as it divides computing time by round-robin scheduling among all active
processes. This breaks their design a bit: ideally the nucleus should
schedule the top-level processes and let those processes schedule their
children themselves. Perhaps it would be too inconvenient if you need to
implement scheduler for every “<span class="smallcaps">os</span>” you
want to run.</p>
<p>Comparing their system, <span class="oldstyle-num"><span class=
"smallcaps">rc400</span></span>, to <span class="smallcaps">the</span>
system, <span class="oldstyle-num"><span class=
"smallcaps">rc400</span></span> uses message passing for communication,
whereas <span class="smallcaps">the</span> system doesn’t have
inter-process communication (it’s a batch system); <span class=
"oldstyle-num"><span class="smallcaps">rc400</span></span> uses mutex for
synchronization and <span class="smallcaps">the</span> system uses
semaphore.</p>
<h2 id="HYDRA" class="section">HYDRA</h2>
<p><em>HYDRA: The Kernel of a Multiprocessor Operating System</em>,
<span class="oldstyle-num">1974</span>.</p>
<p>Their design goals include separation of mechanism and policy, and
rejection of strict hierarchy layering for access control, because they
consider access control more of a net than layers. Their design goal also
emphasized on protection: not only comprehensive protection, but also
flexible protection. They provide protection mechanism that can be used
for not only for regular things like <span class="smallcaps">i/o</span>,
etc, but also arbitrary things that a higher-level program want to
protect/control. It would be nice if <span class="smallcaps">unix</span>
has something similar to offer...</p>
<p><em>Capability</em> is basically the right to use some resource, a key
to a door. Possessing the capability means you have the right of using
whatever resource it references. For example, file descriptors in
<span class="smallcaps">unix</span> are capabilities: when you open a
file, the kernel checks if you are allowed to read/write that file, and
if the check passes, you get a file descriptor. Then you are free to
read/write to that file using the file descriptor. Kernel knows you have
the right since you hold that file descriptor.</p>
<p>In an access-controlled <span class="smallcaps">os</span>, you
have</p>
<ul>
<li>Resources (data, device);</li>
<li>Execution domains (eg, execute as a user); and</li>
<li>access control in domain to resource</li>
</ul>
<p>In <span class="smallcaps">hydra</span>, you have <em>procedure</em>,
<span class="smallcaps">lns</span>, and <em>process</em>. Procedure is
things like a executable program or a subroutine. <span class=
"smallcaps">lns</span> (local name space) is the execution domain.
Conceptually it is a collection of capabilities, ie, it determines what
you can and cannot do. Each invocation of a procedure has a <span class=
"smallcaps">lns</span> attached to it. Paraphrase in <span class=
"smallcaps">unix</span> terms, when a user Alice runs a program
<code>ls</code>, the capabilities Alice has is the <span class=
"smallcaps">lns</span>, and <code>ls</code> is the procedure. Finally, a
process is conceptually a (call) stack of procedures with their
accompanying <span class="smallcaps">lns</span>.</p>
<p>Since each invocation of procedures have an accompanying <span class=
"smallcaps">lns</span>, the callee’s <span class="smallcaps">lns</span>
could have more or different capabilities from its caller, so you can
have things like <em>right amplification</em>.</p>
<p>Right amplification is when caller has more privilege than the caller
(of course, privilege is a <span class="smallcaps">unix</span> concept,
<span class="smallcaps">hydra</span> rejects hierarchy). For example, in
<span class="smallcaps">unix</span>, when a program uses a syscall, that
syscall executed by the kernel has far more privilege than the caller.
For another example (in <span class="smallcaps">unix</span>), when Alice
runs <code>passwd</code> to change her password, that program modifies
the password file which Alice has no access to. In <span class=
"smallcaps">unix</span>, the first example is the result of kernel having
ultimate privilege, and the second example is implemented by euid
(effective user id).</p>
<p>Another concept often used in security (but not in <span class=
"smallcaps">hydra</span>) is <span class="smallcaps">acl</span> (access
control list). It’s basically a table recording who has access to what.
To use an <span class="smallcaps">acl</span>, you need to know the user;
with capabilities, anyone with the capability can have access, you don’t
need to know the particular user—this is firstly easier to check, and
secondly useful for distributed systems or simply very large systems,
where storing information of all users/entities is not possible.</p>
<p>However, capabilities are unforgettable, ie, you can’t take it back.
Maybe you can make them expire, but that’s more complexity. Capabilities
can also be duplicated and given away, which has it’s own virtues and
vices.</p>
<p>Since <span class="smallcaps">acl</span> is easy to store and manage,
and capability is easy to check, they are often used together. In
<span class="smallcaps">unix</span>, opening a file warrens a check in
the <span class="smallcaps">acl</span>, and the file descriptor returned
to you is a capability.</p>
<p>It’s interesting to think of the access control systems used around
us. Windows certainly has a more sophisticated <span class=
"smallcaps">acl</span> than <span class="smallcaps">unix</span>. What
about Google Docs, eh? On top of the regular features, they also support
“accessible through links”, “can comment but not edit”, etc.</p>
<h2 id="TENEX" class="section">TENEX</h2>
<p><em>TENEX, a Paged Time Sharing System for the PDP-10</em>,
<span class="oldstyle-num">1972</span>.</p>
<p><span class="smallcaps">tenex</span> is the predecessor of
<span class="smallcaps">multics</span>, which is the predecessor of
<span class="smallcaps">unix</span>. It runs on <span class=
"smallcaps">pdp-10</span>, a machine very popular at the time: used by
Harvard, <span class="smallcaps">mit</span>, <span class=
"smallcaps">cmu</span>, to name a few. It was manufactured by
<span class="smallcaps">bbn</span>, a military contractor at the time.
<span class="smallcaps">pdp-10</span> is micro-coded, meaning its
instructions are programmable.</p>
<p>In <span class="smallcaps">bbn</span>’s pager, each page is
<span class="oldstyle-num">512</span> words, the <span class=
"smallcaps">tlb</span> is called “associative register”. Their virtual
memory supports <span class="oldstyle-num">256</span>K words and
copy-on-write. A process in <span class="smallcaps">tenex</span> always
have exactly one superior (parent) process and any number of inferior
(child) processes. Processes communicate through (a) sharing memory, (b)
direct control (parent to child only), and (c) pseudo (software
simulated) interrupts. Theses are also the only ways of communication we
have to date in <span class="smallcaps">unix</span>. Would be nice if we
had messages or something...</p>
<p><span class="smallcaps">tenex</span> is able to run binary programs
compiled for <span class="oldstyle-num"><span class="smallcaps">dec
10/50</span></span>, the original <span class="smallcaps">os</span> for
the <span class="oldstyle-num"><span class=
"smallcaps">pdp-10</span></span>. All the <span class=
"smallcaps">tenex</span> syscalls “were implemented with the <span class=
"smallcaps">jsys</span> instruction, reserving all old monitor
[<span class="smallcaps">os</span>/kernel] calls for their previous use”.
They also implemented all the <span class="oldstyle-num"><span class=
"smallcaps">dec 10/50</span></span> syscalls as a compatibility package.
The first time a program calls a <span class="oldstyle-num"><span class=
"smallcaps">dec 10/50</span></span> syscall, that package is mapped “to a
remote portion of the process address space, and area not usually
available ton a <span class="oldstyle-num">10/50</span> system”.</p>
<p><span class="smallcaps">tenex</span> use balanced set scheduling. A
balanced set is a set of highest priority processes whose total working
set fits in memory, where working set means the pages these processes
reference. This way we reduce the page faults. According to my professor,
Linux now uses fault frequency as the measure for scheduling.</p>
<p>Guess what is an “executive command language interpreter”? They
descried it as “...which provides direct access to a large variety of
small, commonly used system functions, and access to and control over all
other subsystems and user programs”. It’s a shell!</p>
<p>Some interesting facts: <span class="smallcaps">tenex</span> supports
at most 5 levels in file paths; the paper mentions file extensions; files
in <span class="smallcaps">tenex</span> are versioned, a new version is
created every time you write to a file, old versions are automatically
deleted by the system; it has five access rights: directory listing,
read, write, execute, and append; they have a debugger residing in the
core memory alongside the kernel.</p>
<p>The file operations is the same as in <span class=
"smallcaps">unix</span>, opening a file gives you a file descriptor,
called <span class="smallcaps">jfn</span> (job file number), and you can
read or write the file. The effect of the write is seen immediately by
readers (so I guess no caching or buffering). They even have “unthawed
access”, meaning only one writer is allowed while multiple reader can
read from the file at the same time. <span class="smallcaps">unix</span>
really cut a lot of corners, didn’t it?</p>
<p>Their conclusion section is also interesting:</p>
<blockquote>
<p>One of the most valuable results of our work was the knowledge we
gained of how to organize a hardware/software project of this size.
Virtually all of the work on TENEX from initial inception to a usable
system was done over a two year period. There were a total of six people
principally involved in the design and implementation. An <span class=
"oldstyle-num">18</span> month part-time study, hardware design and
implementation culminated in a series of documents which describe in
considerable detail each of the important modules of the system. These
documents were carefully and closely followed during the actual coding of
the system. The first state of coding was completed in <span class=
"oldstyle-num">6</span> months; at this point the system was operating
and capable of sustaining use by nonsystem users for work on their
individual projects. The key design document, the JSYS Manual (extended
machine code), was kept updated by a person who devoted full time to
insuring its consistency and coherence; and in retrospect, it is out
judgment that this contributed significantly to the overall integrity of
the system.</p>
<p>We felt it was extremely important to optimize the size of the tasks
and the number of people working on the project. We felt that too many
people working on a particular task or too great an overlap of people on
separate tasks would result in serious inefficiency. Therefore, tasks
given to each person were as large as could reasonably be handled by that
person, and insofar as possible, tasks were independent or related in
ways that were well defined and documented. We believe that this
procedure was a major factor in the demonstrated integrity of the system
as well as in the speed with which it was implemented.</p>
</blockquote>
<h2 id="MULTICS" class="section">MULTICS</h2>
<p><em>Protection and the Control of Information Sharing in Multics</em>,
<span class="oldstyle-num">1974</span>.</p>
<p>The almighty <span class="smallcaps">multics</span>, running on the
equally powerful Honeywell <span class="oldstyle-num">6180</span>. There
are multiple papers on <span class="smallcaps">multic</span>, and this
one is about its protection system.</p>
<p>Their design principles are</p>
<ol>
<li>Permission rather than exclusion (ie, default is no permission)</li>
<li><a id="footref:selinux" class="footref-anchor obviously-a-link"
aria-label="Jump to footnote" href="#footdef%3Aselinux">Check every
access to every object<sup class="inline-footref">2</sup></a></li>
<li>The design is not secret (ie, security not by obscurity)</li>
<li>Principle of least privilege</li>
<li>Easy to use and understand (human interface) is important to reduce
human mistakes</li>
</ol>
<div id="footdef:selinux" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Aselinux">2</a></div>
<div class="def-footdef">Early-day Linux doesn’t do this, which led to
SELinux. It has merged into main Linux long ago.</div>
</div>
<p><span class="smallcaps">multics</span> has a concept of <em>descriptor
segments</em>. The virtual memory is made of segments, and each segment
has a descriptor, containing access right, protection domain, etc. Thus
<span class="smallcaps">multics</span> has access-control for memory. The
access check are done by hardware for speed. That also means <span class=
"smallcaps">multics</span> depends on the hardware and are not portable,
unlike <span class="smallcaps">unix</span>.</p>
<p><span class="smallcaps">multic</span> uses an regular <span class=
"smallcaps">acl</span> for file-access control. When opening a file, the
kernel checks for access rights, creates a segment descriptor
(capability), and maps the whole file into virtual memory as a segment.
In the paper, the <span class="smallcaps">acl</span> is described as the
first level access-control, and the hardware-based access-control the
second level. Note that files in <span class="smallcaps">multics</span>
you can’t read a file as a stream: the whole file is mmaped into memory,
essentially.</p>
<p><span class="smallcaps">multics</span> also has <em>protected
subsystems</em>. It’s a collection of procedure and data that can only be
used through designated entry points called “gates” (think of an
<span class="smallcaps">api</span>). To me, it’s like modules
(public/private functions and variables) in programming languages, but in
an <span class="smallcaps">os</span>. All subsystems are put on a
hierarchy, every subsystem within a process get a number, lower-numbered
subsystems can use descriptors containing higher-numbered subsystems. And
the protection is guaranteed by the hardware. They call it “rings of
protection”.</p>
<p>Speaking of rings, <span class="oldstyle-num">x86</span> supports four
ring levels, this is how kernel protects itself from userspace programs.
Traditionally userspace is on ring <span class="oldstyle-num">3</span>
and kernel is on ring <span class="oldstyle-num">0</span>. Nowadays with
virtual machines, the guest <span class="smallcaps">os</span> is put on
ring <span class="smallcaps">1</span>.</p>
<h2 id="Protection" class="section">Protection</h2>
<p><em>Protection</em>, <span class="oldstyle-num">1974</span>.</p>
<p>This paper by Butler Lampson give an overview of protection in
systems, and introduces a couple useful concepts.</p>
<p>A <em>protection domain</em> is anything that has certain rights to do
something and has some protection from other things, eg, kernel or
userspace, a process, a user. A lot of words are used to describe it:
protection context, environment, state, sphere, capability list, ring,
domain.</p>
<p>Then there are <em>objects</em>, things needs to be protected. Domains
themselves can be objects. The relationship between domains and objects
form a matrix, the <em>access matrix</em>. Each relationship between a
domain and an object can be a list of <em>access attributes</em>, like
owner, control, call, read, write, etc.</p>
<p>When implementing the access matrix, the system might want to attach
the list of accessible object of a domain to that domain. Each element of
this list is essentially a capability.</p>
<p>Alternatively, the system can attach a list of domains that can access
an object to that object. An object would have a procedure that takes a
domain’s name as input and returns its access rights to this object. The
domain’s name shouldn’t be forge-able. One idea is to use capability as
the domain identifier: a domain would ask the supervisor (kernel) for an
identifier (so it can’t be forged), and pass it to objects’
access-control procedure. An arbitrary procedure is often an overkill,
and an <em>access lock list</em> is used instead.</p>
<p>Many system use a hybrid implementation in which a domain first access
an object by access-key to obtain a capability, which is used for
subsequent access. (Opening a file.)</p>
<h2 id="UNIX" class="section">UNIX</h2>
<p><em>The UNIX Time-Sharing System</em>, <span class=
"oldstyle-num">1974</span>.</p>
<p>The good ol’ <span class="smallcaps">unix</span>! This paper describe
the “modern” <span class="smallcaps">unix</span> written in C, running on
<span class="oldstyle-num"><span class=
"smallcaps">pdp-11</span></span>.</p>
<p>Comparing to systems like <span class="smallcaps">tenex</span> and
<span class="smallcaps">multics</span>, <span class=
"smallcaps">unix</span> has a simpler design and does not require special
hardware supports, since it has always been designed for rather limited
machines, and for its creators’ own use only, free from requirements
imposed by other people.</p>
<p>The paper spends major portions describing the file system, something
we tend to take for granted from an operating system, and <a id=
"footref:filesystem" class="footref-anchor obviously-a-link" aria-label=
"Jump to footnote" href="#footdef%3Afilesystem">view as swappable
nowadays<sup class="inline-footref">3</sup></a>. We are all too familiar
with “everything as a file”. <span class="smallcaps">unix</span> treats
files as a linear sequence of bytes, but that’s not the only possible
way. <span class="smallcaps">ibm</span> filesystems has the notion of
“records” <a id="footref:fs-database" class=
"footref-anchor obviously-a-link" aria-label="Jump to footnote" href=
"#footdef%3Afs-database">like in a database<sup class=
"inline-footref">4</sup></a>. And on <span class=
"smallcaps">multics</span>, as we’ve seen, the whole file is <a id=
"footref:fs-mmap" class="footref-anchor obviously-a-link" aria-label=
"Jump to footnote" href="#footdef%3Afs-mmap">mmaped to the
memory<sup class="inline-footref">5</sup></a>.</p>
<div id="footdef:filesystem" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Afilesystem">3</a></div>
<div class="def-footdef">Because most filesystems we use expose the same
interface, namely the <span class="smallcaps">posix</span> standard. They
all have read, write, open, close, seek, makedir, etc. I wish in the
future we can plug in custom filesystems to the <span class=
"smallcaps">os</span> and expose new interfaces for programs to use. For
example, a network filesystem that can tell the program “I’m downloading
this file from remote, the progress is xx%”. Right now network
filesystems either block or immediately error out.</div>
</div>
<div id="footdef:fs-database" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Afs-database">4</a></div>
<div class="def-footdef">As every idea in <span class=
"smallcaps">cs</span>, this might be coming back in another form. For
example, Android uses (modified) SQLite for its filesystem.</div>
</div>
<div id="footdef:fs-mmap" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Afs-mmap">5</a></div>
<div class="def-footdef">Again, this might be coming back, in the form of
persistent memory.</div>
</div>
<p><span class="smallcaps">unix</span> uses mounting to integrate
multiple devices into a single namespace. If you look at <span class=
"smallcaps">ms dos</span>, they use filenames to represent devices.</p>
<p>This version of <span class="smallcaps">unix</span> only has seven
protections bits, one of which switches set-user-id, so there is no
permission for “group”. set-user-id is just the effective user id thing
we talked about earlier in the <span class="smallcaps">hydra</span>
section.</p>
<p>The paper talked about the shell in detail, for example the <code>|
&lt; &gt; ; &</code> operators. Judging from the example, the
<code>&lt;</code> and <code>&gt;</code> are clearly intended to be
prefixes rather than operators:</p>
<pre class="code-block">ls &gt;temp1
pr -2 &lt;temp1 &gt;temp2
opr &lt;temp2</pre>
<h2 id="Plan%209" class="section">Plan 9</h2>
<p><em>Plan 9 From Bell Labs</em>, <span class=
"oldstyle-num">1995</span>.</p>
<p>According to the paper, by the mid <span class=
"oldstyle-num">1980</span>’s, people have moved away from centralized,
powerful timesharing systems (on mainframes and mini-computers) to small
personal micro-computers. But a network of machines have difficulty
serving as seamlessly as the old timesharing system. They want to build a
system that feels like the old timesharing system, but is made of a bunch
of micro-computers. Instead of having a single powerful computer that
does everything, they will have individual micro-computers for each task:
a <a id="footref:computing" class="footref-anchor obviously-a-link"
aria-label="Jump to footnote" href="#footdef%3Acomputing">computing
(<span class="smallcaps">cpu</span>) server<sup class=
"inline-footref">6</sup></a>, a file server, routers, terminals, etc.</p>
<div id="footdef:computing" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Acomputing">6</a></div>
<div class="def-footdef">Of course, this wouldn’t make sense anymore,
since <span class="smallcaps">cpu</span>’s are so much faster than
networks now.</div>
</div>
<p>The central idea is to expose every service as files. Each user can
compose their own private namespace, mapping files, devices, and services
(as files) into a single hierarchy. Finally, all communication are made
through a single protocol, <span class="oldstyle-num"><span class=
"smallcaps">9p</span></span>. Compare that to what we have now, where the
interface is essentially C <span class="smallcaps">abi</span>, it
certainly sounds nice. But I have to say, using text stream as the sole
interface for everything is a bit questionable too.</p>
<p>Their file server has an interesting storage called <span class=
"smallcaps">worm</span> (write-once, read many), it’s basically a time
machine. Everyday at <span class="oldstyle-num">5</span> <span class=
"smallcaps">am</span>, a snapshot of all the disks is taken and put into
the <span class="smallcaps">worm</span> storage. People can get back old
versions of their files by simply reading the <span class=
"smallcaps">worm</span> storage. Nowadays <span class=
"smallcaps">worm</span> snapshot is often used to defend against ransom
attacks. And you better make sure the <span class="smallcaps">worm</span>
is absolutely read-only ;-)</p>
<h2 id="Medusa" class="section">Medusa</h2>
<p><em>Medusa: An Experiment in Distributed Operating Systems
Structure</em>, <span class="oldstyle-num">1980</span>.</p>
<p>A distributed system made at <span class="smallcaps">cmu</span>, to
closely match and maximally exploit its hardware: the
distributed-processor Cm* (computer modules) system.</p>
<p>On a distributed processor hardware, they can place the kernel code in
memory in three ways:</p>
<ol>
<li>Replicate the kernel on every node</li>
<li>Kernel code on one node, other nodes’ processors execute code
remotely</li>
<li>Distributed kernel</li>
</ol>
<p>They chose the third approach: divide the kernel into
<em>utilities</em> (kernel module) and distribute them among all the
processors, “with no guarantee that any particular processor contains a
copy of the code for any particular utility”. When a running program
needs to invoke a certain utility (some syscall provided by some kernel
module), it migrates to the processor that has that utility. Each utility
could be on multiple processors, so programs don’t have to fight for a
single popular utility.</p>
<p>The design is primarily influenced by efficiency (on the given
hardware), but it also has nice structure properties. Boundaries between
utilities are “rigidly enforced”; each utility can only send messages to
each other and not modify other’s memory. This improves security and
robustness (error in one utility won’t affect other utilities).</p>
<p>One problem that might occur when you split the kernel into modules is
circular dependency and, in exertion, deadlocks. If the filesystem
utility calls into the memory manager utility (eg, get a buffer), and the
memory manager utility calls into the filesystem utility (eg, swap
pages), you have a circular dependency. Mix in locks and you might get a
deadlock.</p>
<p>To be deadlock-free, Medusa further divides each utility into
<em>service classes</em> so service classes don’t have circular
dependencies. It also makes sure each utility use separate and
statistically allocated resources, so they don’t step on each other.</p>
<p>Programs written to run on Medusa are mostly concurrent in nature.
Instead of conventional processes, program execution are carried out by
<em>task forces</em>, which is a collection of <em>activities</em>, where
each activity is like a thread but runs on different processors.
Activities access kernel objects (resources like memory page, pipe, file,
etc) through descriptors. Each activity has a <em>private descriptor
list</em> (<span class="smallcaps">pdl</span>), and all activities in a
task force share a <em>shared descriptor list</em> (<span class=
"smallcaps">sdl</span>). There are also <em>utility descriptor list</em>
(<span class="smallcaps">udl</span>) for utility entry points (syscalls),
and <em>external descriptor list</em> (<span class=
"smallcaps">xdl</span>) referencing remote <span class=
"smallcaps">udl</span> and <span class="smallcaps">pdl</span>. Both
<span class="smallcaps">udl</span> and <span class="smallcaps">xdl</span>
are processor-specific.</p>
<p>The task force notion is useful for scheduling: Medusa schedules
activities that are in the same task force to run in the same time. It’s
often referred to as <em>gang scheduling</em> or <em>coscheduling</em>,
where you schedule inter-communicating processes to run together, just
like working sets in paging. In addition, Medusa does not schedule out an
activity immediately when it starts waiting (eg, for a lock), and
spin-waits for a short while (<em>pause time</em>), in the hope that the
wait is short (shorter than context switch).</p>
<p>Utilities stores information for an activity alongside the activity
rather than storing it on the utility’s processor. This way if an
utilities fails, another utility can come in, read the information, and
carry on the work. (Remember that utilities are duplicated on multiple
processors.) The utility <em>seals</em> the information stored with the
activity, so user programs can’t muddle with it. Only other utilities can
unseal and use that information. Implementation wise, unsealing means
mapping the kernel object into the <span class="smallcaps">xdl</span> (of
the processor running the utility), and sealing it means removing it from
the <span class="smallcaps">xdl</span>.</p>
<p>Medusa’s kernel also provide some handy utilities like the exception
reporter and a debugger/tracer. When an exception occurs, the kernel on
the processor sends exception data to the reporter, which sends that
information to other activities (<em>buddy activity</em>) to handle. And
you can use the debugger/tracer to online-debug programs. Must be nice if
the kernel drops you into a debugger when your program segfaults, no?
(Looking at Common Lisp.) I feel that Ken Thompson being too good a
programmer negatively impacted the computing device we have today. If he
isn’t that good, perhaps he would add a kernel debugger in <span class=
"smallcaps">unix</span> ;-)</p>
<h2 id="Pilot" class="section">Pilot</h2>
<p><em>Pilot: An Operating System for a Personal Computer</em>,
<span class="oldstyle-num">1980</span>.</p>
<p>A system developed by Xerox <span class="smallcaps">parc</span> on
their personal work stations. Since it is intended for personal
computing, they made some interesting design decisions. The kernel
doesn’t worry about fairness in allocating resources, and can take
advices from userspace. For example, userspace programs can mark some
process as high priority for <a id="footref:pilot-scheduling" class=
"footref-anchor obviously-a-link" aria-label="Jump to footnote" href=
"#footdef%3Apilot-scheduling">scheduling<sup class=
"inline-footref">7</sup></a>, or pin some pages in the memory so it’s
never swapped out. (These are just examples, I don’t know for sure if you
can do these things in Pilot.)</p>
<div id="footdef:pilot-scheduling" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Apilot-scheduling">7</a></div>
<div class="def-footdef">Very recently we start to see big/small cores in
Apple M1 and Intel 12th gen, and “quality of service” in macOS.</div>
</div>
<p>Pilot uses the same language, Mesa, for operating system and user
programs, which allows the two to coupe tightly.</p>
<p>Pilot provides defense (against errors) but not <a id=
"footref:absolute-protection" class="footref-anchor obviously-a-link"
aria-label="Jump to footnote" href=
"#footdef%3Aabsolute-protection">absolute protection<sup class=
"inline-footref">8</sup></a>. And protection is language-based, provided
by (and only by) type-checking in Mesa, the language used to write both
the operating system and programs.</p>
<div id="footdef:absolute-protection" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href=
"#footref%3Aabsolute-protection">8</a></div>
<div class="def-footdef">This is before Internet, and malicious software
isn’t a thing yet, I think?</div>
</div>
<p>Lastly, Pilot has integrated support for networks. It is designed to
be used in a network (of Pilots). In fact, the first distributed email
system is created on Pilot.</p>
<p>The device on which Pilot runs is also worth noting: a powerful
machine, with high-resolution bitmap display, keyboard, and a “pointing
device”. Xerox <span class="smallcaps">parc</span> basically invented
personal computer, and <span class="smallcaps">gui</span> and mouse that
go with it.</p>
<p>The filesystem is flat (no directory hierarchy), though higher level
software are free to implement additional structure. Files are accessed
through mapping its pages (blocks) into virtual memory. Files and volumes
(devices) are named by a 64-bit unique id (uid), which means files
created anywhere anytime can be uniquely identified across different
machines (and thus across the network). They used a classic trick, unique
serial number plus real-time clock, to guarantee uniqueness.</p>
<p>A file can be marked immutable. An immutable file can’t every be
modified again, and can be shared across machines without changing its
uid. This is useful for, eg, sharing programs.</p>
<h2 id="Monitor" class="section">Monitor</h2>
<p><em>Monitors: An Operating System Structuring Concept</em>,
<span class="oldstyle-num">1974</span>, by C. A. R. Hoare.</p>
<p><em>Experience with Processes and Monitors in Mesa</em>, <span class=
"oldstyle-num">1980</span>.</p>
<p><em>Monitor</em> is a synchronization concept. Think of it as a class
that manages some resource and synchronizes automatically. In C, you
would manually create a mutex and lock/unlock it; in Java, you just add
some keyword in front of a variable and the runtime creates and manages
the lock for you.</p>
<p>The Hoare paper introduced the concept and gave a bunch of examples.
The Mesa paper describes how did they implement and use monitors in Mesa.
If you recall, Mesa is the system and application language for Pilot.</p>
<p>Pilot uses monitors provided by Mesa to implement synchronization in
the kernel, another example of the tight coupling of Pilot and Mesa.</p>
<p>I have some notes on the differences between Mesa’s monitors and
Hoare’s monitors, but they aren’t very interesting. Basically Mesa folks
needed to figure out a lot of details for using monitors for Pilot, eg,
nested wait, creating monitor, handling exceptions in monitor,
scheduling, class level vs instance level, etc.</p>
<p><em>Priority inversion</em> is when a low priority processes holds the
lock, preventing a high priority process from running because the high
priority process needs to acquire the lock. This is a big issue in
real-time systems: maybe an emergency handler needs to run immediately
but can’t acquire a lock. Priority inversion is usually solved by
promoting the low priority process to run and release the lock.</p>
<p>Mesa folks didn’t use mutual monitors between devices. If two devices
with substantial difference in processing speed shares a monitor, the
fast device could be slowed down by waiting for the slower device to
finish its critical section.</p>
<h2 id="V%20Kernel" class="section">V Kernel</h2>
<p><em>The Distributed V Kernel and its Performance for Diskless
Workstations</em>, <span class="oldstyle-num">1983</span>.</p>
<p>Papers we’ve read up to this point are more of a description of the
system the author’s built, their experiences and lessons learned. Back in
the day, professors and their grad students work together to build an
awesome and cutting-edge system, and journals invite them to write down
their thoughts and experiences.</p>
<p>This paper is a bit different, it uses performance measurements to
argue a claim: The conventional approach to build a distributed
workstation is to use a small local disk as cache and use specialized
protocols. This papar tries to build a distributed workstation without
local disks (hence diskless) and with generic message-based <span class=
"smallcaps">ipc</span>, and argue that the overhead added by this two
decisions are ok.</p>
<p>The paper introduces V message, it is synchronous (request and reply),
has a small message size (<span class="oldstyle-num">32</span> bytes),
and has separate control data messages. Thought they also have a
control+data message (ReplyWithSegment), presumably to squeeze out some
performance.</p>
<p>They used a various of measures to reduce the overhead. They put
everything into the kernel, including the file server. They didn’t use
<span class="smallcaps">tcp</span> but used Ethernet frames directly.
There is no separate <span class="smallcaps">ack</span> message, instead
<span class="smallcaps">ack</span> is implied by a response.</p>
<p>The paper analyzed what network penalty consists of. When you send a
message from one host to another, it goes from the <span class=
"smallcaps">ram</span> to the network interface, then is transferred on
wire to the destination interface, then copied into <span class=
"smallcaps">ram</span>. Their argument is that message layer doesn’t add
much overhead comparing to the base network penalty. They also argued
that remote file access adds small overhead comparing to already-slow
disk access.</p>
<p>Overall, their arguments aren’t without flaws. For example, they argue
that there is no need for specialized message protocol, but their
protocol ends up specializing. They also argued that no streaming is
needed, but large data packet are effectively streaming.</p>
<h2 id="Sprite" class="section">Sprite</h2>
<p><em>The Sprite Network Operating System</em>, <span class=
"oldstyle-num">1988</span>.</p>
<p>Sprite is another distributed system. It tries to use large memory
cache to improve file access, tries to be transparent, giving the user
the illusion of a local system. It also has a very cool process migration
feature, even though process migration was never adopted by the
industry.</p>
<p>At the time, several trends influenced Sprite’s design. Distributed
system was very popular (at least in academia); memories are getting
larger and larger; and more and more systems are featuring multiple
processors.</p>
<p>To present the illusion of a local file system, Sprite uses <em>prefix
tables</em>. A prefix is a path prefix. When the userspace accesses a
file, the kernel looks at the prefix table, entries in the prefix table
point to the local filesystem, or to a remote filesystem. If it points to
a remote filesystem, the kernel makes <span class="smallcaps">rpc</span>
calls to the remote host, when ends up reaching the local filesystem of
the remote host. Prefix table isn’t only used by distributed system,
though. Any <span class="smallcaps">os</span> that uses file paths will
cache the directories and files it reads in a prefix table.</p>
<p>With cache, the biggest problem is consistency: if two clients get a
file and stored it in their cache, and both write to their cache, you
have a problem. If only one writer is allowed at a time and the system
tracks the last writer of every file. Files are versioned. When a client
needs to read a file, it finds the last write and requests the file from
it. This is <em>sequential write-sharing</em>.</p>
<p>If multiple clients needs to write the same file (<em>concurrent
write-sharing</em>), Sprite just turns off caching. This is rare enough
to not worth complicating the system. (And you probably need
substantially complication to handle this...)</p>
<h2 id="Grapevine" class="section">Grapevine</h2>
<p><em>Experience with Grapevine: The Growth of a Distributed
System</em>, 1984.</p>
<p>A classic paper in distributed systems, even considered the
<span class="smallcaps">multics</span> of distributed systems by some.
Grapevine is a distributed email delivery and management system, provides
message delivery, naming, authentication, resource location, access
control—you name it.</p>
<p>The main takeaway is the experience they got from running Grapevine.
To support scaling, the cost of any computation/operation should not grow
as the size of thet system grows. But on the other hand, sometimes you
can afford to have complete information—maybe that information can never
get too large, regardless of how large the system grows.</p>
<p>Grapevine generally tries to hide the distributed nature of the
system, but that caused some problem. First of all, they can’t really
hide everything: update in the sytem takes time to propagate, and
sometimes users get duplicated messages, all of which are confusing for
someone accustomed to the mail service on time-sharing systems. More
importantly, user sometimes needs to know more information of the
underlying system to understand what’s going on: when stuff doesn’t work,
people want to know why. For example, removing an inbox is an expensive
operation and removing a lot of them in the same time could overload the
system. System administrators needs to understand this, and to understand
this they to understand roughly how the system works under the hood.</p>
<p>The lesson is, complete transparency is usually note possible, and
often not a good decision either. When you design a system, it is
important to decide what to make transparent and what not to.</p>
<p>Finally, the paper mentioned some considerations about managing the
system. Maintaining a geographically dispersed system involves on-site
operators, who need to be able to carry out operations with little to no
understanding of the underlying system, and system experts, who are in
short supply and almost always remote from almost all servers. Grapevine
has remote monitoring and debugging features so an expert can diagnose
and repair a server remotely.</p>
<figure><img src="./grapevine.jpg" alt=
"The system structure of Grapevine.">
<figcaption>The system structure of Grapevine.</figcaption>
</figure>
<h2 id="Global%20memory" class="section">Global memory</h2>
<p><em>Implementing Global Memory Management in a Workstation
Cluster</em>, <span class="oldstyle-num">1995</span>.</p>
<p>This paper is purely academic, but pretty cool. They built a cluster
that shares physical memory at a very low level, below <span class=
"smallcaps">vm</span>, paging, file-mapping, etc. This allows the system
to utilize the physical memory much better, and allow more file-caching,
which is beneficial since the <span class="smallcaps">cpu</span> was
becoming much more faster than the disk.</p>
<p>Each node in the cluster divides their memory into <em>local
memory</em> and <em>global memory</em>. Local memory stores pages
requested by local processes; global memory stores pages in behave of
other nodes in the cluster.</p>
<p>When a fault occurs on a node P, one of four things could happen.</p>
<ol>
<li>If the requested page is in the global memory of another node Q, P
uses a random page in its global memory to trade the desired page with Q.
(See illustration 1.)</li>
<li>If the requested page is in the global memory of another node Q, but
P doesn’t have any page in its global memory, P use the <span class=
"smallcaps">lru</span> (least-recently used) local page to trade with
Q.</li>
<li>If the requested page is on local disk, read it into P’s local
memory, and evict the oldest page in the <em>entire cluster</em> to make
room for the new page. If the oldest page is on P, evict that; if the
oldest page is on a node Q, evict the page on Q, and send a page of P to
Q. Send a random global page on P, or the <span class=
"smallcaps">lru</span> local page of P if there is no global page on P,
just like in case 1 and 2. (See illustration 2.)</li>
<li>If the requested page is a local page of another node Q, duplicate
that page into the local memory of P, and evict the oldest page in the
entire cluster. Again, if the oldest page is on another node R, send one
of P’s global pages or P’s <span class="smallcaps">lru</span> page.</li>
</ol>
<figure><img src="./global-memory-1.jpg" alt=
"Illustration of page exchange in case 1.">
<figcaption>Illustration 1: Page exchange in case 1.</figcaption>
</figure>
<figure><img src="./global-memory-2.jpg" alt=
"Illustration of page exchange in case 3.">
<figcaption>Illustartion 2: Page exchange in case 3.</figcaption>
</figure>
<p>This whole dance can improve performance of memory-intensive tasks
because fetching a page from remote memory is about two to ten times
faster than disk access. However, local hit is over three magnitudes
faster than fetching remote memory, so the algorithm has to be very
careful not to evict the wrong page.</p>
<p>This algorithm is nice and all, but how do memory management code
running on each node know which page is the oldest page in the entire
cluster?</p>
<p>In the best but impossible scenario, the system is truly managed by a
single entity, a central controller; the controller keeps track of every
single page’s age and tells each node which node to evict. Of course,
this is impossible because that’s way too slow, the controller has to be
running at a much faster speed than the other nodes and the communication
speed between nodes must be very fast.</p>
<p>Instead, each node must make local independent decisions that combines
to achieve a global goal (evict the oldest page). The difficulty is that
local nodes usually don’t have complete, up-to-date information.</p>
<p>The solution to this kind of scenario is probability-based algorithm.
We don’t aim to make the optimal decision for every single case, but use
probability to approximate the optimal outcome, with the incomplete and
out-of-date information each node does know.</p>
<p>We divide time into epochs, in each epoch, the cluster expects to
replace <em>M</em> oldest pages. (<em>M</em> is predicted from date from
previous epochs.) At the beginning of each epoch, every node sens a
summary of its pages and their age to a <em>initiator node</em> (central
controller). The initiator node sorts all the pages by their age, and
find <em>W</em> = the set of <em>M</em> oldest pages in the cluster.
Then, it assigns each node <em>i</em> a weight <em>w<sub>i</sub></em>,
where <em>w<sub>i</sub></em> is</p>
<p><img src="./global-memory-frac.png" alt=
"A math expression: the number of old pages in W that are in node i, divided by W."></p>
<p>Basically, <em>w<sub>i</sub></em> means “among the <em>M</em> oldest
pages in the cluster, how many of them are in node <em>i</em>”.</p>
<p>The initiator node tells each node every node’s weight, and when a
node P encounters case 3 or 4 and wants to evict “the oldest page in the
cluster”, it randomly picks a node by each node’s weight, and tells that
node to <a id="footref:global-mem-tlb" class=
"footref-anchor obviously-a-link" aria-label="Jump to footnote" href=
"#footdef%3Aglobal-mem-tlb">evict its oldest page<sup class=
"inline-footref">9</sup></a>.</p>
<div id="footdef:global-mem-tlb" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Aglobal-mem-tlb">9</a></div>
<div class="def-footdef">
<p>Actually, tracking page age isn’t that simple. For one, in a mmaped
file, memory access bypasses pagefault handler and goes straight to the
<em>tlb</em>. More importantly, the <span class="smallcaps">os</span>
uses <span class="smallcaps">fifo</span> second-chance page caching and
hides many page request/eviction from their memory manager, because the
memory manager runs at a lower level (presumably in pagefault
handlers).</p>
<p>The authors resorted to hacking the <span class="smallcaps">tlb</span>
handler of the machine with PALcode (microcode). This would’ve been
impossible on x86—it’s <span class="smallcaps">tlb</span> is handled
purely in hardware.</p>
</div>
</div>
<p>Probability-based algorithms sometimes feels outright magical—they
seem to just bypass trade-offs. In reality, they usually just add a new
dimension to the trade-off. We’ll see this again later in lottery
scheduling.</p>
<h2 id="%CE%BC-kernel" class="section">μ-kernel</h2>
<p><em>The Performance of μ-Kernel-Based Systems</em>, <span class=
"oldstyle-num">1997</span>.</p>
<p>This paper is a measurement paper. It uses benchmarks to argue that a)
micro kernel can deliver comparable performance, and b) the performance
doesn’t depend on a particular architecture.</p>
<p>The authors built a micro kernel L4, and ported Linux to run on it
(called L⁴Linux). And they ported L4 from Pentium to Alpha and
<span class="smallcaps">mips</span> architecture, to show that L4 is
architecture-independent. They also conducted some experiment to show
L4’s extensibility and performance.</p>
<p>The paper considers micro kernels like Mach and Chrous to be
first-generation, which evolved from earlier monolithic kernels. Later
kernels like L4 and <span class="smallcaps">qnx</span> are considered
second-generation; they are designed more rigorously from scratch, ie,
more “pure”.</p>
<p>L4 allows user programs to control memory allocation, like nucleus
did: kernel manages top-level tasks’ memory, top-level tasks manages
their children’s memory. And scheduling? Hard priorities with round-robin
scheduling per priority, not unlike nucleus.</p>
<p>L⁴Linux only modifies the architecture-dependent part of Linux, ie,
not hacky. The authors also restricted themselves to not make any
Linux-specific change to L4, as a test of the design of L4. The result is
not bad. In micro benchmarks, L⁴Linux is ×<span class=
"oldstyle-num">2.4</span> times slower than native Linux; in macro
benchmarks, L⁴Linux is about <span class="oldstyle-num">5–10%</span>
slower than native Linux. More over L⁴Linux is much faster Linux running
on top of other micro kernels, like MkLinux (Linux + Mach 3.0).</p>
<p>The paper also mentions supporting tagged <span class=
"smallcaps">tlb</span>s. Normal <span class="smallcaps">tlb</span> needs
to be flashed on context switch, which is a main reason why context
switch is expensive. But if you tag each entry in the <span class=
"smallcaps">tlb</span> with a tag to associate that entry with a specific
process, then you don’t need to flush <span class="smallcaps">tlb</span>
anymore. Tagged <span class="smallcaps">tlb</span> needs some form of
software-managed <span class="smallcaps">tlb</span>, after all, hardware
don’t know about processes.</p>
<p>The benefit of micro kernels is of course the extensibility. For
example, when a page is swapped out, instead of writing to disk, we can
swap to a remote machine, or encrypt the page and write to disk, or
compress the page and write to page, etc. A database program could bypass
the filesystem and file cache, and control the layout of data on physical
disk for optimization; it can control caching and keep pages in memory
and not swapped out.</p>
<p>All of these are very nice perks, and the performance doesn’t seem too
bad, then why micro kernels are still not popular? My professor has a
very convencing argument: the big companies can just hire kernel
developers to <a id="footref:mu-kernel-linux" class=
"footref-anchor obviously-a-link" aria-label="Jump to footnote" href=
"#footdef%3Amu-kernel-linux">modify Linux to their need<sup class=
"inline-footref">10</sup></a>, smaller companies don’t have special
requirements for the <span class="smallcaps">os</span> and can just use
Linux. That leaves only the companies in the middle: have special
requirements, but don’t want to modify Linux. And don’t forget that
extending micro kernel is still work, it might be easier than modifying
Linux, but how much easier? If there are a lot of Linux kernel
developers, perhaps modifying Linux is more economical. Eg, Nintendo
Switch and Playstation use their modified <span class=
"smallcaps">bsd</span>, and Steam Deck is built on top of Linux.</p>
<div id="footdef:mu-kernel-linux" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Amu-kernel-linux">10</a></div>
<div class="def-footdef">And they did. Since the paper has been written,
Linux has added many features of L4 described in the paper.</div>
</div>
<p>Beyond monolithic and micro kernel, there are many other designs for
kernel: hybrid, exokernel, even virtual machines. Hybrid kernels include
Windows <span class="smallcaps">nt</span>, NetWave, BeOS, etc. Eg, they
can leave <span class="smallcaps">ipc</span>, driver, <span class=
"smallcaps">vm</span>, and scheduling in the kernel, but put filesystem
in the userspace.</p>
<h2 id="Exokernel" class="section">Exokernel</h2>
<p><em>Exokernel: An Operating System Architecture for Application-Level
Resource Management</em>, <span class="oldstyle-num">1997</span>.</p>
<p>The idea is to go a step further than micro kernels and make the
kernel into a library, the kernel exposes hardware resources and provide
multiplexing and protection, and leaves management to the application.
The motivation is that traditional kernel abstraction hides key
information and obstructs application-specific optimizations.</p>
<p>This idea can be nicely applied to single-purpose applicants, when the
whole purpose of a machine is to run a single application, eg, a
database, a web server, or a embedded application. In this case, thing
that a traditional kernel provides: users, permissions, fairness are all
unnecessary overhead. <a href=
"https://dl.acm.org/doi/10.1145/2490301.2451167">Unikernel</a> explored
exactly this use-case.</p>
<p>Exokernel exports hardware resources and protection, and leaves
management to the (untrusted) application. Applications can request for
resources and handle events securely by <em>secure bindings</em>. Each
application cooperatively share the limited resources by participating in
a <em>resource revocation</em> protocol, ie, the exokernel might tell an
application to release some resources for others to use. Finally, the
exokernel can forcibly retract resources held by uncooperative
applications by the <em>abort protocol</em>.</p>
<p>Exokenel doesn’t provide many of the traditional abstractions, like
<span class="smallcaps">vm</span> or <span class="smallcaps">ipc</span>,
those are left for the application to implement.</p>
<p>The protection provided by an exokernel is inevitably weaker: an
application error could corrupt on-disk data; and because the kernel and
application runs in the same <span class="smallcaps">vm</span>,
application error could corrupt kernel!</p>
<p>The existence of abort protocol kind of breaks the “no management”
principle—retracting resources from an application <em>is</em>
management.</p>
<p>Finally, their benchmark isn’t very convincing: there are only micro
benchmarks and no macro benchmark; they only benchmark mechanism (context
switch, exception handler, etc) but not application.</p>
<h2 id="Xen" class="section">Xen</h2>
<p><em>Xen and the Art of Virtualization</em>, <span class=
"oldstyle-num">2003</span>.</p>
<p>Xen is a <span class="smallcaps">vmm</span>, virtual machine monitor,
also called hypervisor—the thing sits between an <span class=
"smallcaps">os</span> and the hardware. The goal of Xen is to be able to
run hundreds of guest <span class="smallcaps">os</span>’s in the same
time.</p>
<p>Xen provides a virtual machine abstraction
(<em>paravirtualization</em>) rather than a full virtual hardware
(<em>full virtualization</em>). Paravirtualization has better performance
and gives the monitor more control, but requires modification to the
guest <span class="smallcaps">os</span>. On the other hand, full
virtualization monitor, for example VMWare, can work with unmodified
guest <span class="smallcaps">os</span>.</p>
<p>Nowadays there are a plethora of virtual machine solutions, like
VMWare, Hyper-V, VirtualBox, <span class="smallcaps">kvm</span>, Xen, to
name a few; on top of that, there are containers like <span class=
"smallcaps">lxc</span>, docker, etc. They all have different
configuration for hardware, host <span class="smallcaps">os</span>,
<span class="smallcaps">vmm</span>/container engine, guest <span class=
"smallcaps">os</span>, and guest app: The <span class=
"smallcaps">vmm</span> can sit on the host <span class=
"smallcaps">os</span> or directly on the hardware; you can run one guest
<span class="smallcaps">os</span> per app, or run a single guest
<span class="smallcaps">os</span> for multiple apps, etc. And on the old
<span class="smallcaps">ibm</span> and <span class="smallcaps">vms</span>
systems, the <span class="smallcaps">vmm</span> supports both a batch
processing <span class="smallcaps">os</span> and an interactive
<span class="smallcaps">os</span>.</p>
<p>Let’s look at how does Xen virtualize and how does it compare to
VMWare.</p>
<p>Scheduling virtualization: Xen uses the Borrowed Virtual Time
(<span class="smallcaps">bvt</span>) algorithm. This algorithm allows a
guest <span class="smallcaps">os</span> to borrow future execution time
to respond to latency-critical tasks.</p>
<p>Instructions virtualization: Boring instructions like <code>add</code>
can just pass-through to the hardware, but privileged instructions needs
intervention from the monitor.</p>
<p>In Xen, the guest <span class="smallcaps">os</span> is modified so
that it is aware of the <span class="smallcaps">vmm</span>, and instead
of doing privileged task by itself, the guest <span class=
"smallcaps">os</span> delegates the work to the <span class=
"smallcaps">vmm</span> by <em>hypercalls</em>. In VMWare, since they
can’t modify the guest <span class="smallcaps">os</span>, privileged
instructions simply trap into <span class="smallcaps">vmm</span>. If you
remember, we talked about rings in the <span class=
"smallcaps">multics</span> section. On <span class=
"oldstyle-num">x86</span>, The <span class="smallcaps">cpu</span> will
trap if it’s asked to execute a privileged instruction when in a low ring
level.</p>
<p>Memory virtualization: The guest <span class="smallcaps">os</span>
isn’t managing physical memory anymore, though we still call it physical
memory; under it, the <span class="smallcaps">vmm</span> has real access
to the phyiscal memory, often called machine memory.</p>
<p>Then, how is the virtual memory address in the guest <span class=
"smallcaps">os</span> translated into machine memory address?</p>
<p>In Xen, they modify the guest <span class="smallcaps">os</span>, so
the guest <span class="smallcaps">os</span> is aware of the
virtualization. It’s page table can map directly from virtual address to
machine address, and <span class="smallcaps">mmu</span> can just read off
of guest <span class="smallcaps">os</span>’s page table. The <span class=
"smallcaps">vmm</span> just need to verify writes to the page table to
enforce protection.</p>
<p>In VMWare, however, the guest <span class="smallcaps">os</span> is
completely unaware of the <span class="smallcaps">vmm</span>, and its
page table maps from virtual address only to physical address. Also the
guest <span class="smallcaps">os</span> writes to its page table without
bothering to notify anyone. To deal with all this, VMWare maintains a
shadow page table that maps virtual address to actual machine address. It
also use dirty bits to make sure whenever the guest <span class=
"smallcaps">os</span> writs to the page table, it is notified and can
update its shadow page table accordingly. (I forgot exactly how.) And
<span class="smallcaps">mmu</span> reads off the shadow page table.
(Presumably by trapping to <span class="smallcaps">vmm</span> when the
guest <span class="smallcaps">os</span> tries to modify the <span class=
"oldstyle-num"><span class="smallcaps">cr3</span></span> register, and
let <span class="smallcaps">vmm</span> override <span class=
"oldstyle-num"><span class="smallcaps">cr3</span></span> to its shadow
page table?)</p>
<figure><img src="./xen.jpg" alt=
"Diagram illustrating Xen and VMWare’s memory remapping approach.">
<figcaption>Illustration of Xen and VMWare memory
virtualization.</figcaption>
</figure>
<p>Note that VMWare needs all these complication only because
<span class="oldstyle-num">x86</span>’s memory management is completely
hardware-based—the kernel can only point the <span class=
"smallcaps">mmu</span> to the page table and has no other control over
the <span class="smallcaps">mmu</span>. Other “higher-end” architectures
usually support software-managed and tagged <span class=
"smallcaps">tlb</span>.</p>
<p>Xen uses a <em>balloon driver</em> to “squeeze” memory out of the
guest <span class="smallcaps">os</span> when needed. When the
<span class="smallcaps">vmm</span> wants to retract memory from the guest
<span class="smallcaps">os</span>, it enlarges the “balloon”, so the
guest <span class="smallcaps">os</span> automatically gives up memory. A
cute trick.</p>
<h2 id="VMS" class="section">VMS</h2>
<p><em>Virtual Memory Management in VAX/VMS</em>, <span class=
"oldstyle-num">1982</span>.</p>
<p>This paper mainly concerns the implementation of the virtual memory
for the <span class="smallcaps">vms</span>. The <span class=
"smallcaps">vms</span> has to run on a variety of low-end hardware with
small memories and slow <span class="smallcaps">cpu</span>’s, and support
all sorts of uses: real time, timeshared, and batch.</p>
<p><span class="smallcaps">vms</span>’s virtual memory has three regions:
program region, control region and system region. The highest two bits of
an address indicates the region, after that are the regular stuff:
<span class="oldstyle-num">20</span> bits of virtual page number and
<span class="oldstyle-num">8</span> bits of byte offset. The system
region is shared by all processes (kernel stack); program and control
region are process-specific. The paper mentions a trick they used: they
mark the first page in the <span class="smallcaps">vm</span> as no
access, so that uninitialized pointers (pointing to <code>0x0</code>)
cause an exception. Linux’s <span class="smallcaps">vm</span> layout
isn’t much different.</p>
<p><span class="smallcaps">vms</span> uses a process-local page
replacement policy. When a process requests for memory that needs to be
paged in, kernel swaps out a page from the set of pages currently used by
the process (called the <em>resident set</em>). This way a heavily
faulting process can only slow down itself.</p>
<p>When a page is removed from the resident set, it doesn’t go out of the
memory immediately, but is appended to one of two lists: the free page
list if it hasn't been modified, or the modified page list if it has.
When kernel needs a fresh page to swap data in, it takes a page from the
head of the free list. And when kernel decides to write pages back to
paging file (swap file), it takes the page from the head of the modified
list. If a page in these two lists are requested again before it moves to
the head and is consumed, it is pulled out and put into the requesting
process’s resident set. This is basically second chance caching: we keep
the page in the memory for a while before really discarding it, in case
it is used again soon.</p>
<p>Because <span class="smallcaps">vms</span> uses a relatively small
<a id="footref:vms-page" class="footref-anchor obviously-a-link"
aria-label="Jump to footnote" href="#footdef%3Avms-page"><span class=
"oldstyle-num">512</span> byte page size<sup class=
"inline-footref">11</sup></a>, pages causes a lot of <span class=
"smallcaps">i/o</span>, which is obviously not good. To reduce the number
of disk operations, they try to read and write several pages at once
(they call this clustering).</p>
<div id="footdef:vms-page" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Avms-page">11</a></div>
<div class="def-footdef">To be compatible with <span class=
"oldstyle-num"><span class="smallcaps">pdp-11</span></span> and because
of the promise of low-latency semiconductor disk technologies (which
obviously didn’t come true on time, we’ll see this happen a few more
times in other papers ;-).</div>
</div>
<p>The paper also mentions some other nice features, like on-demand
zeroed page and copy-on-reference page. On-demand zeroed page are only
allocated and zeroed when it’s actually referenced. Similarly,
copy-on-reference pages are only copied when it’s actually referenced. I
wonder why didn’t they make it copy-on-write though, they say it’s used
for sharing executable files.</p>
<p>A fun anecdote: our professor once had a grad student doing their
qualifying exam, and one of the council asked: “does the kernel know
about every memory access?” The grad student said yes, and it was wrong.
The council member later complained to my professor that a grad student
can get it wrong. The kernel only get to know about memory use in the
form of pagefault handlers. If there’s no pagefault, the memory access is
handled silently by the <span class="smallcaps">mmu</span>.</p>
<h2 id="Mach" class="section">Mach</h2>
<p><em>Machine-Independent Virtual Memory Management for Paged
Uniprocessor and Multiprocessor Architectures</em>, <span class=
"oldstyle-num">1987</span>.</p>
<p>Mach was a popular research <span class="smallcaps">os</span>. Our
professor did her PhD on Mach’s virtual memory. It actually influenced
both Windows and Mac: one of the prominent Mach research went to Windows
and worked on Windows <span class="smallcaps">nt</span>, and Mac
<span class="smallcaps">osx</span> was Mach plus <span class=
"smallcaps">bsd</span> plus NextStep.</p>
<p>The main point of this paper is machine-independent <span class=
"smallcaps">vm</span> and the idea is to treat hardware information (ie,
machine-dependent, eg, <span class="smallcaps">tlb</span>) as a cache of
machine-independent information.</p>
<p>Unlike <span class="smallcaps">vms</span>, Mach supports sparse
address. Its page table is a sorted doubly linked list of <em>virtual
regions</em>. Each virtual region stores some machine-independent info
like address range, inheritance (we’ll come back to it later),
protection, and some machine-dependent cache info. The machine-dependent
part is a cache and can be re-constructed from the machine-independent
info.</p>
<p>Each virtual region maps a range of virtual addresses to a range in a
<em>memory object</em>. A memory object is an abstraction over a piece of
data/storage, and can even be remote data, I think?</p>
<p>A memory object is associated with a pager, which handles pagefault
and page-out requests. This pager is outside of the kernel and is
customizable. So it can do elaborate things like encrypted memory,
etc.</p>
<p>Copy-on-write is implemented by creating a shadow memory object, which
only contains the written page. The kernel will look for the unmodified
page in its original object. They are just like keymaps in Emacs. Shadow
memory objects themselves can be shadowed, and large chains of shadow
objects will manifest, Mach had to garbage collect intermediate shadow
objects when the chain gets long. Reading from the paper, this is
probably quite an annoyance to the designers.</p>
<p>When a task inherits memory from its parent task, the parent can set
the inheritance flag of any page to one of <em>shared</em> (read-write),
<em>copy</em> (copy-on-write), or <em>none</em> (no access). This would
be
<br>
helpful for sandboxing.</p>
<h2 id="FFS" class="section">FFS</h2>
<p><em>A Fast File System for UNIX</em>, <span class=
"oldstyle-num">1984</span>.</p>
<p>This paper literally describes a faster file system they implemented
for <span class="smallcaps">unix</span>. It was widely adopted.</p>
<p>The author identifies a series of shortcomings of the default file
system of <span class="smallcaps">unix</span>:</p>
<p>The free list (linked list storing all free blocks) starts out
ordered, but over time becomes random, so when the file system allocates
blocks for files, those block are not physically continuous but rather
scatter around.</p>
<p>The inodes are stored in one place, and the data (blocks) another.
File operations (list directory, open, read, write) involve editing meta
information interleaved with writing data, causing long seeks between the
inodes and the blocks.</p>
<p>The block size is 512 bytes, which is too small and creates
indirection and fragmentation. Smaller block size also means it takes
more disk transactions to transfer the same amount of data.</p>
<p>All these combined renders the default file system only able to
produce <span class="oldstyle-num">2%</span> of the full bandwidth.</p>
<p><span class="smallcaps">ffs</span> improves performance by creating
locality as much as possible. It uses a larger block size, decides a disk
partition into <em>cylinder groups</em>. Each cylinder groups has its own
(duplicated) superblock, inodes, and a free list implemented with a
bitmap. This way inodes and data blocks are resonably close to each
other. Each cylinder has a fixed number of inodes.</p>
<p><span class="smallcaps">ffs</span> uses a smart allocation policy when
allocating blocks for files and directories. Eg, it tries to place inodes
of files in the same directory in the same cylinder group; it places new
directories in a cylinder group that has more free inocdes and less
excising directories; it tries to place all data blocks of a file in the
same cylinder group; etc.</p>
<p>Larger block size wastes space, because most <span class=
"smallcaps">unix</span> systems are composed of many small files.
<span class="smallcaps">ffs</span> allows a block to be splitted into
<em>fragments</em>. A block can be broken into 2, 4, or 8 fragments. At
the end, the author claims that <span class="smallcaps">ffs</span> with
4096-byte blocks and 512-byte fragments has about the same disk
utilization as the old 512-byte block file system.</p>
<p><span class="smallcaps">ffs</span> also requires some percent of free
space to maintain it’s performance. Because when the disk is too full,
it’s hard for <span class="smallcaps">ffs</span> to keep the blocks of a
file localized. <span class="smallcaps">ffs</span> performs best when
there are around <span class="oldstyle-num">10%</span> of free space.</p>
<p>To maximally optimize the file system, <span class=
"smallcaps">ffs</span> is parameterized so it can be tuned according to
the physical property of the disk (number of blocks on a track, spin
speed), processor speed (speed of interrupt and disk transfer), etc. Two
physically consecutive blocks on the disk can’t be read consecutively
because there’s some processing time when reading a block of data.
<span class="smallcaps">ffs</span> can calculate the number of blocks to
skip according to processor speed and spin speed, such that when the
<span class="smallcaps">os</span> finished reading one block, the next
block of the file comes into position right under the disk head.</p>
<h2 id="LFS" class="section">LFS</h2>
<p><em>The Design and Implementation of a Log-Structured File
System</em>, <span class="oldstyle-num">1991</span>.</p>
<p>When the paper came out, it stirred quote some controversy on
<span class="smallcaps">lfs</span> vs extend-based <span class=
"smallcaps">ffs</span>. The main complain for <span class=
"smallcaps">lfs</span> is that it needs garbage collection.</p>
<p>The main idea behind <span class="smallcaps">lfs</span> is that now
machines have large <span class="smallcaps">ram</span>, read can just use
cache, and the filesystem can optimize for write instead. To optimize
write, what if we only do sequential write?</p>
</article>
</main>
<footer id="postamble">
<div>
<p>Written by Yuan Fu</p>
<p>Published on 2023-02-15 Wed</p>
<p>Comment by sending a message to <a href=
"mailto:~casouri/public-inbox@lists.sr.ht?Subject=Re%3A%20Notes%20of%20CSE%20221%3A%20Operating%20Systems">
the public inbox</a></p>
<p><a href=
"https://lists.sr.ht/~casouri/public-inbox?search=Notes%20of%20CSE%20221%3A%20Operating%20Systems">
View existing discussions</a> | <a href=
"https://man.sr.ht/lists.sr.ht/etiquette.md">Mailing list
etiquette</a></p>
<p><a href="/note/atom.xml">RSS</a> | <a href=
"https://github.com/casouri/casouri.github.io">Source</a> | <a href=
"https://creativecommons.org/licenses/by-sa/4.0/">License</a></p>
</div>
</footer>
</body>
</html>
