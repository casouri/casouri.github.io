<!DOCTYPE html>
<html lang="en">
<head>
<title>Classic Systems Papers: Notes for CSE 221</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="./../../note-style.css">
<link rel="icon" type="image/png" href="./../../../favicon.png">
</head>
<body>
<header id="header" class="obviously-a-link">
<nav><a href="../../../index.html">Home</a><span>▶︎</span><a href=
"../../index.html">Notes</a><span>▶︎</span></nav>
</header>
<main id="body">
<article>
<div class="title">
<h1 class="main-title">Classic Systems Papers: Notes for CSE 221</h1>
<p class="subtitle">UCSD recap, ep1</p>
</div>
<nav id="toc" class="obviously-a-link">
<h2>Table of Contents</h2>
<ol>
<li><a href="#THE%20System">THE System</a></li>
<li><a href="#Nucleus">Nucleus</a></li>
<li><a href="#HYDRA">HYDRA</a></li>
<li><a href="#TENEX">TENEX</a></li>
<li><a href="#MULTICS">MULTICS</a></li>
<li><a href="#Protection">Protection</a></li>
<li><a href="#UNIX">UNIX</a></li>
<li><a href="#Plan%209">Plan 9</a></li>
<li><a href="#Medusa">Medusa</a></li>
<li><a href="#Pilot">Pilot</a></li>
<li><a href="#Monitor">Monitor</a></li>
<li><a href="#V%20Kernel">V Kernel</a></li>
<li><a href="#Sprite">Sprite</a></li>
<li><a href="#Grapevine">Grapevine</a></li>
<li><a href="#Global%20memory">Global memory</a></li>
<li><a href="#%CE%BC-kernel">μ-kernel</a></li>
<li><a href="#Exokernel">Exokernel</a></li>
<li><a href="#Xen">Xen</a></li>
<li><a href="#VMS">VMS</a></li>
<li><a href="#Mach">Mach</a></li>
<li><a href="#FFS">FFS</a></li>
<li><a href="#LFS">LFS</a></li>
<li><a href="#Soft%20update">Soft update</a></li>
<li><a href="#Rio">Rio</a></li>
<li><a href="#Scheduler%20activation">Scheduler activation</a></li>
<li><a href="#Lottery%20scheduling">Lottery scheduling</a></li>
<li><a href="#Epilogue">Epilogue</a></li>
</ol>
</nav>
<p>During my time at <span class="smallcaps">ucsd</span>, I enjoyed their
systems courses greatly. It’d be a shame to let those wonderful things I
learnt fade away from my memory. So I compiled my notes into this
article. I hope this can be helpful to future me and entertaining for
others.</p>
<p><span class="oldstyle-num"><span class="smallcaps">cse
221</span></span> is the entry course, introducing students to reading
papers and the essential systems papers. The cast of papers is pretty
stable over the years. Here is a syllabus of a <span class=
"oldstyle-num"><span class="smallcaps">cse 221</span></span> similar to
the one I took, you can find links to the papers and extension reading
there: <a href=
"https://cseweb.ucsd.edu/classes/wi21/cse221-a/readings.html"><em>CSE
221: Reading List and Schedule, Winter 2021</em></a>.</p>
<h2 id="THE%20System" class="section">THE System</h2>
<p><em>The Structure of the “THE”-Multiprogramming System</em>,
<span class="oldstyle-num">1968</span>, by none other than Edsger W.
Dijkstra.</p>
<p>The main take away is “central abstraction in a hierarchy”. The
central abstraction is sequential process, and hierarchy is basically
“layers”. The benefit of layers is that it’s easy to verify soundness and
prove correctness for each individual layer, which is essential to handle
complexity.</p>
<p>To Dijkstra, if a designer structures their system well, the possible
test cases for the system at each level would be so few such that it’s
easy to cover every possible case.</p>
<p>He then mentioned that “industrial software makers” has mixed feelings
of this methodology: they agree that it’s the right thing to do, but
doubt whether it’s applicable in the real world, away from the shelter of
academia. Dijkstra’s stance is that the larger the project, the more
essential the structuring. This stance is apparent in <a id=
"footref:ewd1041" class="footref-anchor obviously-a-link" aria-label=
"Jump to footnote" href="#footdef%3Aewd1041">his other
writings<sup class="inline-footref">1</sup></a>.</p>
<div id="footdef:ewd1041" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Aewd1041">1</a></div>
<div class="def-footdef"><a href=
"https://www.cs.utexas.edu/users/EWD/transcriptions/EWD10xx/EWD1041.html">
<em>EWD 1041</em></a>. Now, I don’t think it is realistic to write proofs
for every system you design (and how do you ensure the proof is
correct?), but good structuring, and designing with testing in mind are
certainly essential.</div>
</div>
<p>The downside of layering is, of course, the potential loss of
efficiently, due to either the overhead added by layering, or the lack of
details hidden by lower layers. For example, the graphic subsystem in
win<span class="oldstyle-num">32</span> was moved into the kernel in
<span class="oldstyle-num"><span class="smallcaps">nt4</span></span>,
because there were too many boundary crossing.</p>
<p>And sometimes it’s hard to separate the system into layers at all, eg,
due to circular dependency, etc. For example, in Linux, memory used by
the scheduler is pinned and never page.</p>
<p>We also learned some interesting terminology used at the time;
“harmonious cooperation” means no deadlock, and “deadly embrace” means
deadlock.</p>
<h2 id="Nucleus" class="section">Nucleus</h2>
<p><em>The Nucleus of a Multiprogramming System</em>, <span class=
"oldstyle-num">1970</span>.</p>
<p>Basically they want a “nucleus” (small kernal) that supports multiple
simultaneous operating system implementations. So the user can have their
<span class="smallcaps">os</span> however they want. (Another example of
“mechanism instead of policy”, sort of.) This school of thought would
later reappear on exokernel and micro kernel.</p>
<p>The nucleus provides a scheduler (for process and <span class=
"smallcaps">i/o</span>), communication (messages passing), and primitive
for controlling processes (create, start, stop, remove).</p>
<p>In their design, the parent process is basically the <span class=
"smallcaps">os</span> of their child processes, controlling allocation of
resources for them: starting/stoping them, allocating memory and storage
to them, etc.</p>
<p>However, the parent process doesn’t have full control over their
children: it doesn’t control scheduling for it’s children. Nucleus
handles scheduling; it divides computing time by round-robin scheduling
among all active processes.</p>
<p>A more “complete” abstraction would be having nucleus schedule the
top-level processes and let those processes schedule their children
themselves. Perhaps it would be too inconvenient if you need to implement
scheduler for every “<span class="smallcaps">os</span>” you want to
run.</p>
<h2 id="HYDRA" class="section">HYDRA</h2>
<p><em>HYDRA: The Kernel of a Multiprocessor Operating System</em>,
<span class="oldstyle-num">1974</span>.</p>
<p>The authros have several design goals for <span class=
"smallcaps">hydra</span>: a) separation of mechanism and policy; b)
reject strict hierarchy layering for access control, because they
consider access control more of a mesh than layers; c) an emphasize on
protection—not only comprehensive protection, but also flexible
protection. They provide protection mechanism that can be used for not
only for regular things like <span class="smallcaps">i/o</span>, etc, but
also arbitrary things that a higher-level program want to
protect/control. It surely would be nice if <span class=
"smallcaps">unix</span> has something similar to offer.</p>
<p><span class="smallcaps">hydra</span> structures protection around
<em>capabilities</em>. Capability is basically the right to use some
resource—a key to a door. Possessing the capability means you have the
right of using whatever resource it references. For example, file
descriptors in <span class="smallcaps">unix</span> are capabilities: when
you open a file, the kernel checks if you are allowed to read/write that
file, and if the check passes, you get a file descriptor. Then you are
free to read/write to that file as long as you hold the file descriptor;
no need to go through access checks every time.</p>
<p>In genreal, in an access-controlled <span class="smallcaps">os</span>,
there are resources, like data or a device; execution domains, like
“execute as this user” or “execute as this group”; and access control,
controlling which domain can access which resource.</p>
<p>In <span class="smallcaps">hydra</span>, there is <em>procedure</em>,
<span class="smallcaps">lns</span>, and <em>process</em>. Procedure is a
executable program or a subroutine. <span class="smallcaps">lns</span>
(local name space) is the execution domain. Conceptually it is a
collection of capabilities, it determines what you can and cannot do.
Each invocation of a procedure has a <span class="smallcaps">lns</span>
attached to it. To explain it in <span class="smallcaps">unix</span>
terms, when a user Alice runs a program <code>ls</code>, the capabilities
Alice has is the <span class="smallcaps">lns</span>, and <code>ls</code>
is the procedure. Finally, a process is conceptually a (call) stack of
procedures with their accompanying <span class=
"smallcaps">lns</span>.</p>
<p>Since each invocation of procedures have an accompanying <span class=
"smallcaps">lns</span>, the callee’s <span class="smallcaps">lns</span>
could have more or different capabilities from its caller, so
<span class="smallcaps">hydra</span> can support <em>right
amplification</em>.</p>
<p>Right amplification is when caller has more privilege/capabilities
than the caller. For example, in <span class="smallcaps">unix</span>,
when a program uses a syscall, that syscall executed by the kernel has
far more privilege than the caller. For another example, when Alice runs
<code>passwd</code> to change her password, that program can modify the
password file which Alice has no access to, because <code>passwd</code>
has a euid (effective user id) with higher privilege.</p>
<p>Another concept often used in security is <span class=
"smallcaps">acl</span> (access control list). It’s basically a table
recording who has access to what. <span class="smallcaps">acl</span> and
capabilities each have their pros and cons. To use an <span class=
"smallcaps">acl</span>, you need to know the user; with capabilities,
anyone with the capability can have access, you don’t need to know the
particular user. Capabilities is easier to check, and useful for
distributed systems or very large systems, where storing information of
all users/entities is not possible.</p>
<p>However, capabilities are unforgettable, ie, you can’t take it back.
Maybe you can make them expire, but that’s more complexity. Capabilities
can also be duplicated and given away, which has it’s own virtues and
vices.</p>
<p>Since <span class="smallcaps">acl</span> is easy to store and manage,
and capability is easy to check, they are often used together. In
<span class="smallcaps">unix</span>, opening a file warrens a check in
the <span class="smallcaps">acl</span>, and the file descriptor returned
to you is a capability.</p>
<p>It’s interesting to think of the access control systems used around
us. Windows certainly has a more sophisticated <span class=
"smallcaps">acl</span> than <span class="smallcaps">unix</span>. What
about Google Docs, eh? On top of the regular features, they also support
“accessible through links”, “can comment but not edit”, etc.</p>
<h2 id="TENEX" class="section">TENEX</h2>
<p><em>TENEX, a Paged Time Sharing System for the PDP-10</em>,
<span class="oldstyle-num">1972</span>.</p>
<p><span class="smallcaps">tenex</span> is the predecessor of
<span class="smallcaps">multics</span>, which in turn is the predecessor
of <span class="smallcaps">unix</span>. It runs on <span class=
"oldstyle-num"><span class="smallcaps">pdp-10</span></span>, a machine
very popular at the time: used by Harvard, <span class=
"smallcaps">mit</span>, <span class="smallcaps">cmu</span>, to name a
few. <span class="oldstyle-num"><span class=
"smallcaps">pdp-10</span></span> was manufactured by <span class=
"smallcaps">bbn</span>, a military contractor at the time. It’s
micro-coded, meaning its instructions are programmable.</p>
<p>In <span class="smallcaps">bbn</span>’s pager, each page is
<span class="oldstyle-num">512</span> words, the <span class=
"smallcaps">tlb</span> is called “associative register”. Their virtual
memory supports <span class="oldstyle-num">256</span>K words and
copy-on-write. A process in <span class="smallcaps">tenex</span> always
have exactly one superior (parent) process and any number of inferior
(child) processes. Processes communicate through a) sharing memory, b)
direct control (parent to child only), and c) pseudo (software simulated)
interrupts. Theses are also the only ways of <span class=
"smallcaps">ipc</span> we have today in <span class=
"smallcaps">unix</span>. Would be nice if we had message-passing built-in
to the <span class="smallcaps">os</span>. But maybe D-Bus is even better,
since it can be portable.</p>
<p><span class="smallcaps">tenex</span> can run binary programs compiled
for <span class="oldstyle-num"><span class="smallcaps">dec
10/50</span></span>, the vendor <span class="smallcaps">os</span> for the
<span class="oldstyle-num"><span class="smallcaps">pdp-10</span></span>.
All the <span class="smallcaps">tenex</span> syscalls “were implemented
with the <span class="smallcaps">jsys</span> instruction, reserving all
old monitor [<span class="smallcaps">os</span>/kernel] calls for their
previous use”. They also implemented all of the <span class=
"oldstyle-num"><span class="smallcaps">dec 10/50</span></span> syscalls
as a compatibility package. The first time a program calls a <span class=
"oldstyle-num"><span class="smallcaps">dec 10/50</span></span> syscall,
that package is mapped “to a remote portion of the process address space,
an area not usually available on a <span class=
"oldstyle-num">10/50</span> system”.</p>
<p><span class="smallcaps">tenex</span> uses balanced set scheduling to
reduce pagefaults. A balanced set is a set of highest priority processes
whose total working set fits in memory. And the working set of a process
is the set of pages this process reference.
<br>
Guess what is an “executive command language interpreter”? They descried
it as “...which provides direct access to a large variety of small,
commonly used system functions, and access to and control over all other
subsystems and user programs”. It’s a shell!</p>
<p>Some other interesting facts: <span class="smallcaps">tenex</span>
supports at most 5 levels in file paths; the paper mentions file
extensions; files in <span class="smallcaps">tenex</span> are versioned,
a new version is created every time you write to a file, old versions are
automatically garbage collected by the system over time; <span class=
"smallcaps">tenex</span> has five access rights: directory listing, read,
write, execute, and append; <span class="smallcaps">tenex</span> also has
a debugger residing in the core memory alongside the kernel.</p>
<p>The file operations is the same as in <span class=
"smallcaps">unix</span>, opening a file gives you a file descriptor,
called <span class="smallcaps">jfn</span> (job file number), and you can
read or write the file. The effect of the write is seen immediately by
readers (so I guess no caching or buffering). They even have “unthawed
access”, meaning only one writer is allowed while multiple reader can
read from the file at the same time. <span class="smallcaps">unix</span>
really cut a lot of corners, didn’t it?</p>
<details>
<summary>Their conclusion section is also interesting…</summary>
<blockquote>
<p>One of the most valuable results of our work was the knowledge we
gained of how to organize a hardware/software project of this size.
Virtually all of the work on TENEX from initial inception to a usable
system was done over a two year period. There were a total of six people
principally involved in the design and implementation. An <span class=
"oldstyle-num">18</span> month part-time study, hardware design and
implementation culminated in a series of documents which describe in
considerable detail each of the important modules of the system. These
documents were carefully and closely followed during the actual coding of
the system. The first state of coding was completed in <span class=
"oldstyle-num">6</span> months; at this point the system was operating
and capable of sustaining use by nonsystem users for work on their
individual projects. The key design document, the JSYS Manual (extended
machine code), was kept updated by a person who devoted full time to
insuring its consistency and coherence; and in retrospect, it is out
judgment that this contributed significantly to the overall integrity of
the system.</p>
<p>We felt it was extremely important to optimize the size of the tasks
and the number of people working on the project. We felt that too many
people working on a particular task or too great an overlap of people on
separate tasks would result in serious inefficiency. Therefore, tasks
given to each person were as large as could reasonably be handled by that
person, and insofar as possible, tasks were independent or related in
ways that were well defined and documented. We believe that this
procedure was a major factor in the demonstrated integrity of the system
as well as in the speed with which it was implemented.</p>
</blockquote>
</details>
<h2 id="MULTICS" class="section">MULTICS</h2>
<p><em>Protection and the Control of Information Sharing in Multics</em>,
<span class="oldstyle-num">1974</span>.</p>
<p>The almighty <span class="smallcaps">multics</span>, running on the
equally powerful Honeywell <span class="oldstyle-num">6180</span>. There
are multiple papers on <span class="smallcaps">multic</span>, and this
one is about its protection system.</p>
<p>Their design principles are</p>
<ol>
<li>Permission rather than exclusion (ie, default is no permission)</li>
<li><a id="footref:selinux" class="footref-anchor obviously-a-link"
aria-label="Jump to footnote" href="#footdef%3Aselinux">Check every
access to every object<sup class="inline-footref">2</sup></a></li>
<li>The design is not secret (ie, security not by obscurity)</li>
<li>Principle of least privilege</li>
<li>Easy to use and understand (human interface) is important to reduce
human mistakes</li>
</ol>
<div id="footdef:selinux" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Aselinux">2</a></div>
<div class="def-footdef">Early-day Linux doesn’t do this, which led to
SELinux. It has merged into main Linux long ago.</div>
</div>
<p><span class="smallcaps">multics</span> has a concept of <em>descriptor
segments</em>. The virtual memory is made of segments, and each segment
has a descriptor, which contains access-control information: access
right, protection domain, etc. This way, <span class=
"smallcaps">multics</span> can access-control memory. The access check
are done by hardware for performance. (Which means <span class=
"smallcaps">multics</span> depends on the hardware and isn’t portable
like <span class="smallcaps">unix</span>).</p>
<p><span class="smallcaps">multic</span> uses an regular <span class=
"smallcaps">acl</span> for file-access-control. When opening a file, the
kernel checks for access rights, creates a segment descriptor, and maps
the whole file into virtual memory as a segment. In the paper, the
<span class="smallcaps">acl</span> is described as the first level
access-control, and the hardware-based access-control the second. Note
that in <span class="smallcaps">multics</span>, you can’t read a file as
a stream: the whole file is mmaped into memory, essentially.</p>
<p><span class="smallcaps">multics</span> also has <em>protected
subsystems</em>. It’s a collection of procedure and data that can only be
used through designated entry points called “gates” (think of an
<span class="smallcaps">api</span>). To me, it’s like modules
(public/private functions and variables) in programming languages, but in
an <span class="smallcaps">os</span>. All subsystems are put in a
hierarchy: Every subsystem within a process gets a number, lower-numbered
subsystems can use descriptors containing higher-numbered subsystems. And
the protection is guaranteed by the hardware. They call it “rings of
protection”.</p>
<p>Speaking of rings, <span class="oldstyle-num">x86</span> supports four
ring levels, this is how kernel protects itself from userspace programs.
Traditionally userspace is on ring <span class="oldstyle-num">3</span>
and kernel is on ring <span class="oldstyle-num">0</span>. Nowadays with
virtual machines, the guest <span class="smallcaps">os</span> is put on
ring <span class="smallcaps">1</span>.</p>
<h2 id="Protection" class="section">Protection</h2>
<p><em>Protection</em>, <span class="oldstyle-num">1974</span>.</p>
<p>This paper by Butler Lampson gave an overview of protection in
systems, and introduces a couple useful concepts.</p>
<p>A <em>protection domain</em> is anything that has certain rights to do
something and has some protection from other things, eg, kernel or
userspace, a process, a user. A lot of words are used to describe it:
protection context, environment, state, sphere, capability list, ring,
domain. Then there are <em>objects</em>, things needs to be protected.
Domains themselves can be objects.</p>
<p>The relationship between domains and objects form a matrix, the
<em>access matrix</em>. Each relationship between a domain and an object
can be a list of <em>access attributes</em>, like owner, control, call,
read, write, etc.</p>
<p>When implementing the access matrix, the system might want to attach
the list of accessible object of a domain to that domain. Each element of
this list is essentially a capability.</p>
<p>Alternatively, the system can attach a list of domains that can access
an object to that object. An object would have a procedure that takes a
domain’s name as input and returns its access rights to this object. The
domain’s name shouldn’t be forge-able. One idea is to use capability as
the domain identifier: a domain would ask the supervisor (kernel) for an
identifier (so it can’t be forged), and pass it to objects’
access-control procedure. An arbitrary procedure is often an overkill,
and an <em>access lock list</em> is used instead.</p>
<p>Many system use a hybrid implementation in which a domain first access
an object by access-key to obtain a capability, which is used for
subsequent access. (Eg, opening a file and geting a file descriptor.)</p>
<h2 id="UNIX" class="section">UNIX</h2>
<p><em>The UNIX Time-Sharing System</em>, <span class=
"oldstyle-num">1974</span>.</p>
<p>The good ol’ <span class="smallcaps">unix</span>! This paper describe
the “modern” <span class="smallcaps">unix</span> written in C, running on
<span class="oldstyle-num"><span class=
"smallcaps">pdp-11</span></span>.</p>
<p>Comparing to systems like <span class="smallcaps">tenex</span> and
<span class="smallcaps">multics</span>, <span class=
"smallcaps">unix</span> has a simpler design and does not require special
hardware supports, since it has always been designed for rather limited
machines, and for its creators’ own use only.</p>
<p>The paper spends major portions describing the file system, something
we tend to take for granted from an operating system and <a id=
"footref:filesystem" class="footref-anchor obviously-a-link" aria-label=
"Jump to footnote" href="#footdef%3Afilesystem">view as swappable
nowadays<sup class="inline-footref">3</sup></a>. We are all too familiar
with “everything as a file”. <span class="smallcaps">unix</span> treats
files as a linear sequence of bytes, but that’s not the only possible
way. <span class="smallcaps">ibm</span> filesystems has the notion of
“records” <a id="footref:fs-database" class=
"footref-anchor obviously-a-link" aria-label="Jump to footnote" href=
"#footdef%3Afs-database">like in a database<sup class=
"inline-footref">4</sup></a>. And on <span class=
"smallcaps">multics</span>, as we’ve seen, the whole file is <a id=
"footref:fs-mmap" class="footref-anchor obviously-a-link" aria-label=
"Jump to footnote" href="#footdef%3Afs-mmap">mmaped to the
memory<sup class="inline-footref">5</sup></a>.</p>
<div id="footdef:filesystem" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Afilesystem">3</a></div>
<div class="def-footdef">Because most filesystems we use expose the same
interface, namely the <span class="smallcaps">posix</span> standard. They
all have read, write, open, close, seek, makedir, etc. I wish in the
future we can plug in custom filesystems to the <span class=
"smallcaps">os</span> and expose new interfaces for programs to use. For
example, a network filesystem that can tell the program “I’m downloading
this file from remote, the progress is xx%”. Right now network
filesystems can only choose between blocking and immediately error
out.</div>
</div>
<div id="footdef:fs-database" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Afs-database">4</a></div>
<div class="def-footdef">As every idea in <span class=
"smallcaps">cs</span>, this might be coming back in another form. For
example, Android uses (modified) SQLite for its filesystem.</div>
</div>
<div id="footdef:fs-mmap" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Afs-mmap">5</a></div>
<div class="def-footdef">Again, this might be coming back, in the form of
persistent memory.</div>
</div>
<p><span class="smallcaps">unix</span> uses mounting to integrate
multiple devices into a single namespace. On the other hand, <span class=
"smallcaps">ms dos</span> uses filenames to represent devices.</p>
<p>This version of <span class="smallcaps">unix</span> only has seven
protections bits, one of which switches set-user-id, so there is no
permission for “group”. set-user-id is essentially effective user id
(euid).</p>
<p>The paper talked about the shell in detail, for example the <code>|
&lt; &gt; ; &</code> operators. Judging from the example, the
<code>&lt;</code> and <code>&gt;</code> are clearly intended to be
prefixes rather than operators (that was one of the mysteries for me
before reading this paper):</p>
<pre class="code-block">ls &gt;temp1
pr -2 &lt;temp1 &gt;temp2
opr &lt;temp2</pre>
<h2 id="Plan%209" class="section">Plan 9</h2>
<p><em>Plan 9 From Bell Labs</em>, <span class=
"oldstyle-num">1995</span>.</p>
<p>According to the paper, by the mid <span class=
"oldstyle-num">1980</span>’s, people have moved away from centralized,
powerful timesharing systems (on mainframes and mini-computers) to small
personal micro-computers. But a network of machines have difficulty
serving as seamlessly as the old timesharing system. They want to build a
system that feels like the old timesharing system, but is made of a bunch
of micro-computers. Instead of having a single powerful computer that
does everything, they will have individual micro-computers for each task:
a computing (<span class="smallcaps">cpu</span>) server, a file server,
routers, terminals, etc.</p>
<p>The central idea is to expose every service as files. Each user can
compose their own private namespace, mapping files, devices, and services
(as files) into a single hierarchy. Finally, all communication are made
through a single protocol, <span class="oldstyle-num"><span class=
"smallcaps">9p</span></span>. Compare that to what we have now, where the
interface is essentially C <span class="smallcaps">abi</span> plus web
<span class="smallcaps">api</span>, it certainly sounds nice. But on the
other hand, using text stream as the sole interface for everything feels
a bit shaky.</p>
<p>Their file server has an interesting storage called <span class=
"smallcaps">worm</span> (write-once, read many), it’s basically a time
machine. Everyday at <span class="oldstyle-num">5</span> <span class=
"smallcaps">am</span>, a snapshot of all the disks is taken and put into
the <span class="smallcaps">worm</span> storage. People can get back old
versions of their files by simply reading the <span class=
"smallcaps">worm</span> storage. Nowadays <span class=
"smallcaps">worm</span> snapshot is often used to defend against ransom
attacks.</p>
<h2 id="Medusa" class="section">Medusa</h2>
<p><em>Medusa: An Experiment in Distributed Operating Systems
Structure</em>, <span class="oldstyle-num">1980</span>.</p>
<p>A distributed system made at <span class="smallcaps">cmu</span>, to
closely match and maximally exploit its hardware: the
distributed-processor Cm* system (Computer Modules).</p>
<p>On a distributed processor hardware, they can place the kernel code in
memory in three ways:</p>
<ol>
<li>Replicate the kernel on every node</li>
<li>Kernel code on one node, other nodes’ processors execute code
remotely</li>
<li>Split the kernel onto multiple nodes</li>
</ol>
<p>They chose the third approach: divide the kernel into
<em>utilities</em> (kernel module) and distribute them among all the
processors. When a running program needs to invoke a certain utility
(basically some syscall provided by some kernel module), it migrates to
the processor that has that utility. Different processors can have the
same utility, so programs don’t have to fight for a single popular
utility.</p>
<p>The design is primarily influenced by efficiency given their
particular hardware, not structural purity, but some nice structure
properties nonetheless arised. Boundaries between utilities are rigidly
enforced, since each utility can only send messages to each other and
can’t modify other’s memory. This improves security and robustness. For
example, error in one utility won’t affect other utilities.</p>
<p>One problem that might occur when you split the kernel into modules is
circular dependency and deadlocks. If the filesystem utility calls into
the memory manager utility (eg, get a buffer), and the memory manager
utility calls into the filesystem utility (eg, swap pages), you have a
circular dependency. Mix in locks and you might get a deadlock.</p>
<p>To be deadlock-free, Medusa further divides each utility into
<em>service classes</em> such that service classes don’t have circular
dependencies between each other. It also makes sure each utility use
separate and statistically allocated resources.</p>
<p>Programs written to run on Medusa are mostly concurrent in nature.
Instead of conventional processes, program execution are carried out by
<em>task forces</em>, which is a collection of <em>activities</em>. Each
activity is like a thread but runs on different processors.</p>
<p>Activities access kernel objects (resources like memory page, pipe,
file, etc) through descriptors. Each activity has a <em>private
descriptor list</em> (<span class="smallcaps">pdl</span>), and all
activities in a task force share a <em>shared descriptor list</em>
(<span class="smallcaps">sdl</span>). There are also <em>utility
descriptor list</em> (<span class="smallcaps">udl</span>) for utility
entry points (syscalls), and <em>external descriptor list</em>
(<span class="smallcaps">xdl</span>) referencing remote <span class=
"smallcaps">udl</span> and <span class="smallcaps">pdl</span>. Both
<span class="smallcaps">udl</span> and <span class="smallcaps">xdl</span>
are processor-specific.</p>
<p>The task force notion is useful for scheduling: Medusa schedules
activities that are in the same task force to run in the same time. It’s
often referred to as <em>gang scheduling</em> or <em>coscheduling</em>,
where you schedule inter-communicating processes to run together, just
like working sets in paging. In addition, Medusa does not schedule out an
activity immediately when it starts waiting, and instead spin-waits for a
short while (<em>pause time</em>), in the hope that the wait is short
(shorter than context switch).</p>
<p>Utilities store information for an activity alongside the activity,
instead of storing it on the utility’s processor. This way if an
utilities fails, another utility can come in, read the information, and
carry on the work. The utility <em>seals</em> the information stored with
the activity, so user programs can’t muddle with it. Only other utilities
can unseal and use that information. Implementation wise, unsealing means
mapping the kernel object into the <span class="smallcaps">xdl</span> of
the processor running the utility; sealing it means removing it from the
<span class="smallcaps">xdl</span>.</p>
<p>Medusa’s kernel also provide some handy utilities like the exception
reporter and a debugger/tracer. When an exception occurs, the kernel on
the processor sends exception data to the reporter, which sends that
information to other activities (<em>buddy activity</em>) to handle. And
you can use the debugger/tracer to online-debug programs. <a id=
"footref:common-lisp" class="footref-anchor obviously-a-link" aria-label=
"Jump to footnote" href="#footdef%3Acommon-lisp">Must be nice if the
kernel drops you into a debugger when your program segfaults,
no?<sup class="inline-footref">6</sup></a> I feel that Ken Thompson being
too good a programmer negatively impacted the capability of computing
devices we have today. If he wasn’t that good, perhaps they would add a
kernel debugger in <span class="smallcaps">unix</span> ;-)</p>
<div id="footdef:common-lisp" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Acommon-lisp">6</a></div>
<div class="def-footdef">Common Lisp can do that, just sayin.</div>
</div>
<h2 id="Pilot" class="section">Pilot</h2>
<p><em>Pilot: An Operating System for a Personal Computer</em>,
<span class="oldstyle-num">1980</span>.</p>
<p>A system developed by Xerox <span class="smallcaps">parc</span> on
their personal work stations. Since it is intended for personal
computing, they made some interesting design choices. The kernel doesn’t
worry about fairness in allocating resources, and can take advices from
userspace. For example, userspace programs can mark some process as high
priority for <a id="footref:pilot-scheduling" class=
"footref-anchor obviously-a-link" aria-label="Jump to footnote" href=
"#footdef%3Apilot-scheduling">scheduling<sup class=
"inline-footref">7</sup></a>, or pin some pages in the memory so it’s
never swapped out. (These are just examples, I don’t know for sure if you
can do these things in Pilot.)</p>
<div id="footdef:pilot-scheduling" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Apilot-scheduling">7</a></div>
<div class="def-footdef">Recently we start to see big/small cores in
Apple M1 and Intel 12th gen, and “quality of service” in macOS.</div>
</div>
<p>Pilot uses the same language, Mesa, for operating system and user
programs. In result, the <span class="smallcaps">os</span> and user
programs are tightly coupled.</p>
<p>Pilot provides defense (against errors) but not <a id=
"footref:absolute-protection" class="footref-anchor obviously-a-link"
aria-label="Jump to footnote" href=
"#footdef%3Aabsolute-protection">absolute protection<sup class=
"inline-footref">8</sup></a>. And protection is language-based, provided
by (and only by) type-checking in Mesa.</p>
<div id="footdef:absolute-protection" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href=
"#footref%3Aabsolute-protection">8</a></div>
<div class="def-footdef">This is before Internet, and malicious program
isn’t a thing yet, I think?</div>
</div>
<p>Lastly, Pilot has integrated support for networks. It is designed to
be used in a network (of Pilots). In fact, the first distributed email
system is created on Pilot.</p>
<p>The device on which Pilot runs is also worth noting. ’Twas a powerful
machine, with high-resolution bitmap display, keyboard, and a “pointing
device”. Xerox <span class="smallcaps">parc</span> basically invented
personal computer, plus <span class="smallcaps">gui</span> and mouse.</p>
<p>The filesystem is flat (no directory hierarchy), though higher level
software are free to implement additional structure. Files are accessed
through mapping its pages (blocks) into virtual memory. Files and volumes
(devices) are named by a 64-bit unique id (uid), which means files
created anywhere anytime can be uniquely identified across different
machines (and thus across the network). They used a classic trick, unique
serial number plus real-time clock, to guarantee uniqueness.</p>
<p>A file can be marked immutable. An immutable file can’t be modified
ever again, and can be shared across machines without changing its uid.
This is useful for, eg, sharing programs.</p>
<h2 id="Monitor" class="section">Monitor</h2>
<p><em>Monitors: An Operating System Structuring Concept</em>,
<span class="oldstyle-num">1974</span>, by C. A. R. Hoare.</p>
<p><em>Experience with Processes and Monitors in Mesa</em>, <span class=
"oldstyle-num">1980</span>.</p>
<p><em>Monitor</em> is a synchronization concept. Think of it as a class
that manages some resource and synchronizes automatically. In C, you
would manually create a mutex and lock/unlock it; in Java, you just add
some keyword in front of a variable and the runtime creates and manages
the lock for you—that’s a monitor.</p>
<p>The Hoare paper introduced the concept and gave a bunch of examples.
The Mesa paper describes how did they implement and use monitors in Mesa.
If you recall, Mesa is the system and application language for Pilot.</p>
<p>Pilot uses monitors provided by Mesa to implement synchronization in
the kernel, another example of the tight coupling of Pilot and Mesa.</p>
<p>I have some notes on the differences between Mesa’s monitors and
Hoare’s monitors, but they aren’t very interesting. Basically Mesa folks
needed to figure out a lot of details for using monitors for Pilot, like
nested wait, creating monitor, handling exceptions in monitor,
scheduling, class level vs instance level, etc.</p>
<p>Pilot didn’t use mutual monitors between devices. If two devices with
orders of magnitude difference in processing speed shares a monitor, the
fast device could be slowed down by waiting for the slower device to
finish its critical section.</p>
<h2 id="V%20Kernel" class="section">V Kernel</h2>
<p><em>The Distributed V Kernel and its Performance for Diskless
Workstations</em>, <span class="oldstyle-num">1983</span>.</p>
<p>Back in the day, professors and their grad students work together to
build an awesome and cutting-edge system, and journals invite them to
write down their thoughts and experiences. Papers we’ve read up to this
point are mostly describing the system the authors built, and sharing
their experiences and lessons learned.</p>
<p>This paper is a bit different—it presents performance measurements and
use it to argue a claim. You see, the conventional approach to build a
distributed workstation is to use a small local disk for caching, and
these systems usually use specialized protocols. This papar tries to
build a distributed workstation without local disks (diskless) and only
use generic message-based <span class="smallcaps">ipc</span>. The authors
argue that the overhead added by this two decisions are ok.</p>
<p>The paper introduced V message. It’s synchronous (request and
response), has a small message size (<span class="oldstyle-num">32</span>
bytes), and has separate control data messages. Though they also have a
“control+data message” (<code>ReplyWithSegment</code>), presumably to
squeeze out some performance.</p>
<p>They used various measures to reduce the overhead. They put everything
into the kernel, including the file server. They didn’t use <span class=
"smallcaps">tcp</span> but used Ethernet frames directly. There is no
separate <span class="smallcaps">ack</span> message, instead <span class=
"smallcaps">ack</span> is implied by a response.</p>
<p>The paper analyzed what network penalty consists of. When you send a
message from one host to another, it goes from <span class=
"smallcaps">ram</span> to the network interface, then it’s transferred on
wire to the destination interface, then it’s copied into <span class=
"smallcaps">ram</span>. Their argument is that message layer doesn’t add
much overhead comparing to the base network penalty—copying between
<span class="smallcaps">ram</span> and network interface, and waiting in
the interface before going onto the wire. They also argued that remote
file access adds little overhead comparing to already-slow disk
access.</p>
<p>Overall, their argument do have some cracks. For example, they argue
that there is no need for specialized message protocol, but their
protocol ends up specializing. They also argued that no streaming is
needed, but large data packet are effectively streaming.</p>
<h2 id="Sprite" class="section">Sprite</h2>
<p><em>The Sprite Network Operating System</em>, <span class=
"oldstyle-num">1988</span>.</p>
<p>Sprite is another distributed system. It tries to use large memory
cache to improve file access; and do it transparently, giving the user
the illusion of a local system. It also has a very cool process migration
feature. Sadly, process migration never caught up in the industry.</p>
<p>Several trends at the time influenced Sprite’s design. Distributed
system was popular (at least in academia); memories are getting larger
and larger; and more and more systems are featuring multiple
processors.</p>
<p>To present the illusion of a local file system, Sprite uses <em>prefix
tables</em>. Here, prefix means path prefix. When the userspace access a
file, the kernel looks for a prefix of the path that’s in the prefix
table. In the prefix table, the prefix can either point to the local
filesystem or a remote filesystem. If it points to a remote filesystem,
the kernel makes <span class="smallcaps">rpc</span> calls to the remote
host, which then access the local filesystem of that remote host.</p>
<p>Prefix table isn’t only useful for distributed system. In general,
<span class="smallcaps">os</span> that uses file paths usually cache the
file paths it reads in a prefix table, because resolving a file path is
very slow. When the <span class="smallcaps">os</span> resolves a file
path, it needs to read each directory in the path to find the next
directory.</p>
<p>With cache, the biggest issue is consistency: if two clients get a
file and stored it in their cache, and both write to their cache, you
have a problem. Sprite’s solution is to allow only one writer at a time
and track the current writer of every file. When a client needs to read a
file, it finds the current writer and requests the file from it. This is
<em>sequential write-sharing</em>.</p>
<p>If multiple clients needs to write the same file (<em>concurrent
write-sharing</em>), Sprite just turns off caching. This is rare enough
to not worth complicating the system. (And you probably need a
substantially more complicated system to handle this.)</p>
<h2 id="Grapevine" class="section">Grapevine</h2>
<p><em>Experience with Grapevine: The Growth of a Distributed
System</em>, <span class="oldstyle-num">1984</span>.</p>
<p>A classic paper in distributed systems, even considered the
<span class="smallcaps">multics</span> of distributed systems by some.
Grapevine is a distributed email delivery and management system; it
provides message delivery, naming, authentication, resource location,
access control—you name it.</p>
<p>The main takeaway is the experience they got from running Grapevine.
To support scaling, the cost of any computation/operation should not grow
as the size of the system grows. But on the other hand, sometimes you can
afford to have complete information—maybe that information can never get
too large, regardless of how large the system grows.</p>
<p>Grapevine generally tries to hide the distributed nature of the
system, but that caused some problem. First of all, they can’t really
hide everything: update in the sytem takes time to propagate, and
sometimes users get duplicated messages, all of which are confusing for
someone accustomed to the mail service on time-sharing systems.</p>
<p>More importantly, user sometimes needs to know more information of the
underlying system to understand what’s going on: when stuff doesn’t work,
people want to know why. For example, removing an inbox is an expensive
operation and removing a lot of them in the same time could overload the
system. System administrators needs to understand this, and to understand
this they need to understand roughly how the system works under the
hood.</p>
<p>The lesson is, complete transparency is usually not possible, and
often not what you want anyway. When you design a system, it is important
to decide what to make transparent and what not to.</p>
<p>Finally, the paper mentioned some considerations about managing the
system. Maintaining a geographically dispersed system involves on-site
operators and system experts. On-site operators carry out operations
on-site, but has little to no understanding of the underlying system.
System experts has deep understanding of the system, but are in short
supply and are almost always remote from the servers they need to work
on. Grapevine has remote monitoring and debugging features to help an
expert to diagnose and repair a server remotely.</p>
<figure><img src="./grapevine.jpg" alt=
"The system structure of Grapevine.">
<figcaption>The system structure of Grapevine.</figcaption>
</figure>
<h2 id="Global%20memory" class="section">Global memory</h2>
<p><em>Implementing Global Memory Management in a Workstation
Cluster</em>, <span class="oldstyle-num">1995</span>.</p>
<p>This paper is purely academic, but pretty cool nonetheless. They built
a cluster that shares physical memory at a very low level, below
<span class="smallcaps">vm</span>, paging, file-mapping, etc. This allows
the system to utilize the physical memory much better and allows more
file-caching. More file caches is nice because <span class=
"smallcaps">cpu</span> was becoming much faster than the disk.</p>
<p>Each node in the cluster divides their memory into local memory and
global memory. Local memory stores pages requested by local processes;
global memory stores pages in behave of other nodes in the cluster.</p>
<p>When a fault occurs on a node P, one of four things could happen.</p>
<ol>
<li>If the requested page is in the global memory of another node Q, P
uses a random page in its global memory to trade the desired page with Q.
(See illustration 1.)</li>
<li>If the requested page is in the global memory of another node Q, but
P doesn’t have any page in its global memory, P use the least-recently
used (<span class="smallcaps">lru</span>) local page to trade with
Q.</li>
<li>If the requested page is on local disk, P reads it into its local
memory, and evict the oldest page in the <em>entire cluster</em> to make
room for the new page. If the oldest page is on P, evict that; if the
oldest page is on a node Q, evict the page on Q, and send a page of P to
Q. This page is either a random global page on P, or the <span class=
"smallcaps">lru</span> local page of P if P has no global pages. (See
illustration 2.)</li>
<li>If the requested page is a local page of another node Q, duplicate
that page into the local memory of P, and evict the oldest page in the
entire cluster. Again, if the oldest page is on another node R, send one
of P’s global pages or P’s <span class="smallcaps">lru</span> page to
trade with R.</li>
</ol>
<figure><img src="./global-memory-1.jpg" alt=
"Illustration of page exchange in case 1.">
<figcaption>Illustration 1: Page exchange in case 1.</figcaption>
</figure>
<figure><img src="./global-memory-2.jpg" alt=
"Illustration of page exchange in case 3.">
<figcaption>Illustartion 2: Page exchange in case 3.</figcaption>
</figure>
<p>This whole dance can improve performance of memory-intensive tasks
because fetching a page from remote memory is about two to ten times
faster than disk access. However, local hit is over three magnitudes
faster than fetching remote memory, so the algorithm has to be very
careful not to evict the wrong page.</p>
<p>The description above omits a crucial problem: how does memory
management code running on each node know which page is the oldest page
in the entire cluster?</p>
<p>Consider the naive solution, where the system is managed by a single
entity, a central controller. The controller keeps track of every single
page’s age and tells each node which node to evict. Of course, this is
impossible because that’s way too slow, the controller has to be running
at a much faster speed than the other nodes and the communication speed
between nodes must be very fast.</p>
<p>Instead, each node must make local independent decisions that combines
to achieve a global goal (evict the oldest page). The difficulty is that
local nodes usually don’t have complete, up-to-date information.</p>
<p>A beautiful approach to this kind of problem is probability-based
algorithm. We don’t aim to make the optimal decision for every single
case, but use probability to approximate the optimal outcome.</p>
<p>We divide time into epochs, in each epoch, the cluster expects to
replace <em>m</em> oldest pages. (<em>m</em> is predicted from date from
previous epochs.) At the beginning of each epoch, every node sends a
summary of its pages and their age to an <em>initiator node</em> (central
controller). The initiator node sorts all the pages by their age, and
finds the set of <em>m</em> oldest pages in the cluster (call it
<em>W</em>). Then, it assigns each node <em>i</em> a weight
<em>w<sub>i</sub></em>, where <em>w<sub>i</sub></em> is</p>
<p><img src="./global-memory-frac.png" alt=
"A math expression: the number of old pages in W that are in node i, divided by W."></p>
<p>Basically, <em>w<sub>i</sub></em> means “among the <em>M</em> oldest
pages in the cluster, how many of them are in node <em>i</em>”.</p>
<p>The initiator node tells each node of every node’s weight, and when a
node P encounters case 3 or 4 and wants to evict “the oldest page in the
cluster”, it randomly picks a node by each node’s weight, and tells that
node to evict its oldest page.</p>
<p>That takes care of finding which node to evict pages from, but
tracking page age isn’t easy either. For one, in a mmaped file, memory
access bypasses pagefault handler and goes straight to the <span class=
"smallcaps">tlb</span>. More importantly, the <span class=
"smallcaps">os</span> uses <span class="smallcaps">fifo</span>
second-chance page caching and hides many page request/eviction from
their memory manager, because the memory manager runs at a lower level
(presumably in pagefault handlers).</p>
<p>The authors resorted to hacking the <span class="smallcaps">tlb</span>
handler of the machine with PALcode (microcode). This would’ve been
impossible on x86—it’s <span class="smallcaps">tlb</span> is handled
purely in hardware.</p>
<p>Probability-based algorithms sometimes feels outright magical—they
seem to just bypass trade-offs. In reality, they usually just add a new
dimension to the trade-off. We’ll see this again later in lottery
scheduling.</p>
<h2 id="%CE%BC-kernel" class="section">μ-kernel</h2>
<p><em>The Performance of μ-Kernel-Based Systems</em>, <span class=
"oldstyle-num">1997</span>.</p>
<p>This paper is another measurement paper. It uses benchmarks to argue
that a) micro kernel can deliver comparable performance, and b) the
performance doesn’t depend on a particular hardware architecture.</p>
<p>The authors built a micro kernel L4, and ported Linux to run on it
(called L⁴Linux). Then they ported L4 itself from Pentium to both Alpha
and <span class="smallcaps">mips</span> architecture—to show that L4 is
architecture-independent. They also conducted some experiment to show
L4’s extensibility and performance.</p>
<p>The paper considers micro kernels like Mach and Chrous to be
first-generation, evolved out of earlier monolithic kernels. It considers
later kernels like L4 and <span class="smallcaps">qnx</span> to be
second-generation, for that they are designed more rigorously from
scratch, ie, more “pure”.</p>
<p>L4 allows user programs to control memory allocation like nucleus did:
kernel manages top-level tasks’ memory, top-level tasks manages their
children’s memory. And scheduling? Hard priorities with round-robin
scheduling per priority, not unlike nucleus.</p>
<p>L⁴Linux only modifies the architecture-dependent part of Linux,
meaning they didn’t have to modify Linux. The authors also restricted
themselves to not make any Linux-specific change to L4, as a test for the
design of L4. The result is not bad: in micro benchmarks, L⁴Linux is
×<span class="oldstyle-num">2.4</span> times slower than native Linux; in
macro benchmarks, L⁴Linux is about <span class=
"oldstyle-num">5–10%</span> slower than native Linux. More over, L⁴Linux
is much faster than running Linux on top of other micro kernels, like
MkLinux (Linux + Mach 3.0).</p>
<p>The paper also mentions supporting tagged <span class=
"smallcaps">tlb</span>s. Normal <span class="smallcaps">tlb</span> needs
to be flashed on context switch, which is a big reason why context switch
is expensive. But if you tag each entry in the <span class=
"smallcaps">tlb</span> with a tag to associate that entry with a specific
process, you wouldn’t need to flush <span class="smallcaps">tlb</span>
anymore. The downside is that, tagged <span class="smallcaps">tlb</span>
needs some form of software-managed <span class="smallcaps">tlb</span>,
so not all architecture can support it. For example, x86 doesn’t support
software-managed <span class="smallcaps">tlb</span>.</p>
<p>The benefit of micro kernels is of course the extensibility. For
example, when a page is swapped out, instead of writing to disk, we can
swap to a remote machine, or encrypt the page and write to disk, or
compress the page and write to page. A database program could bypass the
filesystem and file cache, and control the layout of data on physical
disk for optimization; it can control caching and keep pages in memory
and not swapped out.</p>
<p>All of these are very nice perks, and the performance doesn’t seem too
bad, then why micro kernels never caught on? Here’s our professor’s take:
big companies can just hire kernel developers to <a id=
"footref:mu-kernel-linux" class="footref-anchor obviously-a-link"
aria-label="Jump to footnote" href="#footdef%3Amu-kernel-linux">modify
Linux to their need<sup class="inline-footref">9</sup></a>; smaller
companies don’t have special requirements and can just use Linux. That
leaves only the companies in the middle: have special requirements, but
don’t want to modify Linux. (Professor’s take ends here.) However,
extending micro kernel is still work, it might be easier than modifying
Linux, but how much easier? Plus, if there are a lot of Linux kernel
developers, perhaps modifying Linux is more easier afterall.</p>
<div id="footdef:mu-kernel-linux" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Amu-kernel-linux">9</a></div>
<div class="def-footdef">And they did. Since the paper has been written,
Linux has gained many features of L4 described in the paper.</div>
</div>
<p>If we look at “we need a custom <span class="smallcaps">os</span>”
scenario today, Nintendo Switch and Playstation use modified <span class=
"smallcaps">bsd</span>, Steam Deck is built on top of Linux. And I’m sure
most data centers run some form of Linux.</p>
<p>Beyond monolithic and microkernel, there are many other kernel
designs: hybrid, exokernel, even virtual machines. Hybrid kernels include
Windows <span class="smallcaps">nt</span>, NetWave, BeOS, etc. Hybrid
kernel leaves some modules in the kernel, like <span class=
"smallcaps">ipc</span>, driver, <span class="smallcaps">vm</span>,
scheduling, and put others in the userspace, like filesystem.</p>
<h2 id="Exokernel" class="section">Exokernel</h2>
<p><em>Exokernel: An Operating System Architecture for Application-Level
Resource Management</em>, <span class="oldstyle-num">1997</span>.</p>
<p>The idea is to go one step further than microkernels and turn the
kernel into a library. Kernel exposes hardware resources, provide
multiplexing and protection, but leaves management to the application.
The motivation is that traditional kernel abstraction hides key
information and obstructs application-specific optimizations.</p>
<p>This idea can be nicely applied to single-purpose applicants, when the
whole purpose of a machine is to run a single application, eg, a
database, a web server, or an embedded program. In this case, things that
a traditional kernel provides like users, permissions, fairness, are all
unnecessary overhead. (<a href=
"https://dl.acm.org/doi/10.1145/2490301.2451167">Unikernel</a> explored
exactly this use-case.)</p>
<p>Exokernel exports hardware resources and protection, and leaves
management to the (untrusted) application. Applications can request for
resources and handle events. Each application cooperatively share the
limited resources by participating in a <em>resource revocation</em>
protocol. Eg, the exokernel might tell an application to release some
resources for others to use. Finally, the exokernel can forcibly retract
resources held by uncooperative applications by the <em>abort
protocol</em>.</p>
<p>Exokenel doesn’t provide many of the traditional abstractions, like
<span class="smallcaps">vm</span> or <span class="smallcaps">ipc</span>,
those are left for the application to implement.</p>
<p>The protection provided by an exokernel is inevitably weaker: an
application error could corrupt on-disk data; and because the kernel and
application runs in the same <span class="smallcaps">vm</span>,
application error could corrupt kernel memory!</p>
<p>The existence of abort protocol kind of breaks the “no management”
principle—retracting resources from an application <em>is</em>
management.</p>
<p>Finally, their benchmark isn’t very convincing: there are only micro
benchmarks and no macro benchmark; they only benchmarked mechanism
(context switch, exception handler, etc) and has no benchmark for
application.</p>
<h2 id="Xen" class="section">Xen</h2>
<p><em>Xen and the Art of Virtualization</em>, <span class=
"oldstyle-num">2003</span>.</p>
<p>Xen is a virtual machine monitor (<span class="smallcaps">vmm</span>),
also called hypervisor—the thing that sits between an <span class=
"smallcaps">os</span> and the hardware. The goal of Xen is to be able to
run hundreds of guest <span class="smallcaps">os</span>’s in the same
time.</p>
<p>Xen provides a virtual machine abstraction
(<em>paravirtualization</em>) rather than a full virtual hardware
(<em>full virtualization</em>). Paravirtualization has better performance
and gives the <span class="smallcaps">vmm</span> more control, but
requires modification to the guest <span class="smallcaps">os</span>. On
the other hand, full virtualization <span class="smallcaps">vmm</span>,
for example VMWare, can work with unmodified guest <span class=
"smallcaps">os</span>.</p>
<p>Nowadays there are a plethora of virtual machine solutions, like
VMWare, Hyper-V, VirtualBox, <span class="smallcaps">kvm</span>, Xen. On
top of that, there are containers like <span class=
"smallcaps">lxc</span>, docker, etc. The whole stack contains
<span class="smallcaps">os</span>, <span class=
"smallcaps">vmm</span>/container engine, guest <span class=
"smallcaps">os</span>, and guest app. These solutions all have different
configurations: The <span class="smallcaps">vmm</span> can sit on the
host <span class="smallcaps">os</span> or directly on the hardware; you
can run one guest <span class="smallcaps">os</span> per app, or run a
single guest <span class="smallcaps">os</span> for multiple apps; on the
old <span class="smallcaps">ibm</span> and <span class=
"smallcaps">vms</span> systems, the <span class="smallcaps">vmm</span>
supports both a batch processing <span class="smallcaps">os</span> and an
interactive <span class="smallcaps">os</span>.</p>
<p>Let’s look at how does Xen virtualize and how does it compare to
VMWare.</p>
<p>Scheduling virtualization: Xen uses the Borrowed Virtual Time
(<span class="smallcaps">bvt</span>) algorithm. This algorithm allows a
guest <span class="smallcaps">os</span> to borrow future execution time
to respond to latency-critical tasks.</p>
<p>Instructions virtualization: Boring instructions like <code>add</code>
can just pass-through to the hardware, but privileged instructions (like
memory access) needs intervention from the monitor.</p>
<p>In Xen, the guest <span class="smallcaps">os</span> is modified so
that it is aware of the <span class="smallcaps">vmm</span>, and instead
of doing privileged task by itself, the guest <span class=
"smallcaps">os</span> delegates the work to the <span class=
"smallcaps">vmm</span> by <em>hypercalls</em>. In VMWare, since they
can’t modify the guest <span class="smallcaps">os</span>, privileged
instructions simply trap into <span class="smallcaps">vmm</span>. If you
remember, we talked about rings in the <span class=
"smallcaps">multics</span> section. On <span class=
"oldstyle-num">x86</span>, The <span class="smallcaps">cpu</span> will
trap if it’s asked to execute a privileged instruction when in a low ring
level.</p>
<p>Memory virtualization: The guest <span class="smallcaps">os</span>
isn’t managing physical memory anymore, though we still call it physical
memory. <span class="smallcaps">vmm</span> has real access to the
phyiscal memory, often called machine memory.</p>
<p>Then, how is the virtual memory address in the guest <span class=
"smallcaps">os</span> translated into machine memory address?</p>
<p>In Xen, the guest <span class="smallcaps">os</span> is aware of the
virtualization. It’s page table can map directly from virtual address to
machine address, and <span class="smallcaps">mmu</span> can just read off
of guest <span class="smallcaps">os</span>’s page table. The <span class=
"smallcaps">vmm</span> just need to verify writes to the page table to
enforce protection.</p>
<p>In VMWare, however, the guest <span class="smallcaps">os</span> is
unaware of the <span class="smallcaps">vmm</span>, and its page table
maps from virtual address to physical address. Also, the guest
<span class="smallcaps">os</span> writes to its page table without
bothering to notify anyone. <span class="smallcaps">vmm</span> maintains
a shadow page table that maps virtual address to actual machine address.
It also uses dirty bits to make sure whenever the guest <span class=
"smallcaps">os</span> writs to the page table, it is notified and can
update its shadow page table accordingly. (I forgot exactly how.) And
<span class="smallcaps">mmu</span> reads off the shadow page table.
(Presumably by trapping to <span class="smallcaps">vmm</span> when the
guest <span class="smallcaps">os</span> tries to modify the <span class=
"oldstyle-num"><span class="smallcaps">cr3</span></span> register, and
let <span class="smallcaps">vmm</span> override <span class=
"oldstyle-num"><span class="smallcaps">cr3</span></span> to its shadow
page table?)</p>
<figure><img src="./xen.jpg" alt=
"Diagram illustrating Xen and VMWare’s memory remapping approach.">
<figcaption>Illustration of Xen and VMWare’s memory
virtualization.</figcaption>
</figure>
<p>Note that VMWare needs all these complication only because
<span class="oldstyle-num">x86</span>’s memory management is completely
hardware-based—the kernel can only point the <span class=
"smallcaps">mmu</span> to the page table and has no other control over
the <span class="smallcaps">mmu</span>. Other “higher-end” architectures
usually support software-managed and tagged <span class=
"smallcaps">tlb</span>.</p>
<p>A clever trick that Xen uses is <em>balloon driver</em>. It’s a drive
whose whole purpose is to take up memory. When the <span class=
"smallcaps">vmm</span> wants to retract memory from the guest
<span class="smallcaps">os</span>, it enlarges the “balloon”, so the
guest <span class="smallcaps">os</span> relinquishes memory to the
host.</p>
<h2 id="VMS" class="section">VMS</h2>
<p><em>Virtual Memory Management in VAX/VMS</em>, <span class=
"oldstyle-num">1982</span>.</p>
<p>This paper mainly concerns of the implementation of the virtual memory
for <span class="smallcaps">vms</span>. <span class=
"smallcaps">vms</span> has to run on a variety of low-end hardware with
small memory and slow <span class="smallcaps">cpu</span>; it also needs
to support drastically different use-cases: real time, timeshared, and
batch. These requirements all affected the design of <span class=
"smallcaps">vms</span>.</p>
<p><span class="smallcaps">vms</span>’s virtual memory has three regions:
program region, control region and system region. The highest two bits of
an address indicates the region, after that are the regular stuff:
<span class="oldstyle-num">20</span> bits of virtual page number and
<span class="oldstyle-num">8</span> bits of byte offset. The system
region (think of it as kernel stack) is shared by all processes; program
and control region are process-specific.</p>
<p>The paper mentions a trick they used: they mark the first page in the
<span class="smallcaps">vm</span> as no access, so that an uninitialized
pointer (pointing to <code>0x0</code>) causes an exception. I think Linux
does the same.</p>
<p><span class="smallcaps">vms</span> uses a process-local page
replacement policy. When a process requests for memory that needs to be
paged in, kernel swaps out a page from this process’s resident set—the
set of pages currently used by that process. This way a heavily paging
process can only slow down itself.</p>
<p>When a page is removed from the resident set, it doesn’t go out of the
memory immediately; instead, it’s appended to one of two lists. It goes
to the free page list if it hasn’t been modified; otherwise it goes to
the modified page list. When kernel needs a fresh page to swap data in,
it takes a page from the head of the free list. When kernel decides to
write pages back to paging file (swap file), it takes the page from the
head of the modified list.</p>
<p>So a page is appended to the end of the list, and gradually moves to
the head, until it’s consumed. But if the page is requested again by the
process while still in the list, it is pulled out and put back into the
the process’s resident set. This is basically second chance caching: we
keep the page in the memory for a while before really discarding it, in
case it is used again soon.</p>
<p>Because <span class="smallcaps">vms</span> uses a relatively small
<a id="footref:vms-page" class="footref-anchor obviously-a-link"
aria-label="Jump to footnote" href="#footdef%3Avms-page"><span class=
"oldstyle-num">512</span> byte page size<sup class=
"inline-footref">10</sup></a>, pages causes a lot of <span class=
"smallcaps">i/o</span>, which is obviously not good. To reduce the number
of disk operations, they try to read and write several pages at once
(they call this clustering).</p>
<div id="footdef:vms-page" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Avms-page">10</a></div>
<div class="def-footdef">To be compatible with <span class=
"oldstyle-num"><span class="smallcaps">pdp-11</span></span> and because
of the promise of low-latency semiconductor disk technologies (which
obviously didn’t materialize on time).</div>
</div>
<p>The paper also mentions some other nice features, like on-demand
zeroed page, and copy-on-reference page. On-demand zeroed page are only
allocated and zeroed when it’s actually referenced. Similarly,
copy-on-reference pages are only copied when it’s actually referenced. I
wonder why didn’t they make it copy-on-write though, they say it’s used
for sharing executable files.</p>
<p>Quiz time: does kernel know about every memory access?</p>
<p>…The answer is no. Kernel only get to know about memory use when
there’s a pagefault, which runs the pagefault handler provided by the
kernel. If there’s no pagefault, memory access is handled silently by the
<span class="smallcaps">mmu</span>.</p>
<h2 id="Mach" class="section">Mach</h2>
<p><em>Machine-Independent Virtual Memory Management for Paged
Uniprocessor and Multiprocessor Architectures</em>, <span class=
"oldstyle-num">1987</span>.</p>
<p>Mach was a popular research <span class="smallcaps">os</span>. In
fact, our professor, Dr. Zhou, did her PhD on Mach’s virtual memory. Mach
actually influenced both Windows and Mac: one of the prominent Mach
researcher went to Microsoft and worked on Windows <span class=
"smallcaps">nt</span>, and Mac <span class="smallcaps">osx</span> was
Mach plus <span class="smallcaps">bsd</span> plus NextStep.</p>
<p>The main topic of this paper is machine-independent <span class=
"smallcaps">vm</span>. The idea is to treat hardware information
(machine-dependent, like <span class="smallcaps">tlb</span>) as a cache
of machine-independent information.</p>
<p>Mach’s page table is a sorted doubly linked list of <em>virtual
regions</em>. Each virtual region stores some machine-independent info
like address range, inheritance, protection, and some cache for the
machine-dependent info. The machine-dependent part is a cache because it
can be re-constructed from the machine-independent info. Also, since Mach
uses doubly linked list, it can support sparse addresses (<span class=
"smallcaps">vms</span> can’t).</p>
<p>Each virtual region maps a virtual address range to a range in a
<em>memory object</em>. A memory object is an abstraction over some data;
it can be a piece of memory, secondary storage, and even remote data, I
think?</p>
<p>A memory object is associated with a pager, which handles pagefault
and page-out requests. This pager is outside of the kernel and is
customizable. And we can make it do interesting things like encrypting
memory, remote memory, etc.</p>
<p>When performing a copy-on-write, Mac creates a shadow memory object
which only contains pages that have been modified. Access to the
unmodified page will be redirected to the original memory object. Since
shadow memory objects themselves can be shadowed, sometimes, large chains
of shadow objects will manifest. Mach has to garbage collect intermediate
shadow objects when the chain gets long. Reading the paper, this seems to
be tricky to implement and was quite an annoyance to the designers.</p>
<p>When a task inherits memory from its parent task, the parent can set
the inheritance flag of any page to either <em>shared</em> (read-write),
<em>copy</em> (copy-on-write), or <em>none</em> (no access). To me, this
would be very helpful for sandboxing.</p>
<h2 id="FFS" class="section">FFS</h2>
<p><em>A Fast File System for UNIX</em>, <span class=
"oldstyle-num">1984</span>.</p>
<p>This paper literally describes a faster file system they implemented
for <span class="smallcaps">unix</span>. It was widely adopted.</p>
<p>The author identifies a series of shortcomings of the default file
system of <span class="smallcaps">unix</span>:</p>
<p>The free list (a linked list of all free blocks) starts out ordered,
but over time becomes random, so when the file system allocates blocks
for files, those block are not physically continuous but rather scatter
around.</p>
<p>The inodes are stored in one place, and the data (blocks) another.
File operations (list directory, open, read, write) involve editing meta
information interleaved with writing data, causing long seeks between the
inodes and the blocks.</p>
<p>The default block size of 512 bytes is too small and creates
indirection and fragmentation. Smaller block size also means it takes
more disk transactions to transfer the same amount of data.</p>
<p>With all these combined, the default file system can only produce
<span class="oldstyle-num">2%</span> of the full bandwidth.</p>
<p><span class="smallcaps">ffs</span> improves performance by creating
locality as much as possible. It divides a disk partition into
<em>cylinder groups</em>. Each cylinder group has its own copy of the
superblock, its own inodes, and a free list implemented with a bitmap.
This way inodes and data blocks are reasonably close to each other. Each
cylinder has a fixed number of inodes.</p>
<p><span class="smallcaps">ffs</span> uses a smart allocation policy for
allocating blocks for files and directories. It tries to place inodes of
files in the same directory in the same cylinder group; it places new
directories in a cylinder group that has more free inocdes and less
existing directories; it tries to place all the data blocks of a file in
the same cylinder group. Basically, anthing that improves locality.</p>
<p><span class="smallcaps">ffs</span> uses a larger block size since 512
bytes is too small. But larger block size wastes space—most <span class=
"smallcaps">unix</span> systems are composed of many small files that
would be smaller than a larger block size. <span class=
"smallcaps">ffs</span> allows a block to be splitted into
<em>fragments</em>. A block can be broken into 2, 4, or 8 fragments. At
the end, the author claims that <span class="smallcaps">ffs</span> with
4096-byte blocks and 512-byte fragments has about the same disk
utilization as the old 512-byte block file system.</p>
<p><span class="smallcaps">ffs</span> requires some percent of free space
to maintain it’s performance. When the disk is too full, it’s hard for
<span class="smallcaps">ffs</span> to keep the blocks of a file
localized. <span class="smallcaps">ffs</span> performs best when there
are around <span class="oldstyle-num">10%</span> of free space. This
applies to most modern filesystems too.</p>
<p>To maximally optimize the file system, <span class=
"smallcaps">ffs</span> is parameterized so it can be tuned according to
the physical property of the disk (number of blocks on a track, spin
speed), processor speed (speed of interrupt and disk transfer), etc.</p>
<p>Here’s one example of how these information could improve performance.
Two physically consecutive blocks on the disk can’t be read
consecutively, because it takes some time for the processor to process
the data after reading a block. <span class="smallcaps">ffs</span> can
calculate the number of blocks to skip according to the processor speed
and spin speed, such that when the <span class="smallcaps">os</span>
finished reading one block, the next block of the file comes into
position right under the disk head.</p>
<h2 id="LFS" class="section">LFS</h2>
<p><em>The Design and Implementation of a Log-Structured File
System</em>, <span class="oldstyle-num">1991</span>.</p>
<p>When this paper came out, it stirred quote some controversy on
<span class="smallcaps">lfs</span> vs extent-based <span class=
"smallcaps">ffs</span>. Comparing to <span class="smallcaps">ffs</span>,
<span class="smallcaps">lfs</span> has much faster writes, but it has
slower read and needs garbage collection.</p>
<p>The main idea is this: since now machines have large <span class=
"smallcaps">ram</span>s, file cache should ensure read is fast; so the
filesystem should optimize for write speed. To optimize write speed, we
can buffer writes in the file cache and write them all at once
sequentially.</p>
<p>This approach solves several shortcoming of <span class=
"smallcaps">ffs</span>. In <span class="smallcaps">ffs</span>, even
though inodes are close to the data, they are still separate and requires
seeking when writing. And the same goes for directories and files. The
typical work load of the filesystem alternates between writing metadata
and data, producing a lot of separate small writes. Further, most of the
files are small, so most writes are really writing metadata. Writing
metadata is much slower than writing files, because the filesystem has to
do synchronous write for metadata, to ensure consistency in case of
unexpected failure (power outage, etc).</p>
<p>On the other hand, <span class="smallcaps">lfs</span> treats the whole
disk as an append-only log. When writing a file, the filssytem just
appends what it wants to write to the end of the log, followed by the new
inodes pointing to the newly written blocks, followed by the new inode
map pointing to the newly written inodes. The inode map is additionally
copied in the memory for fast access.</p>
<p>To read, <span class="smallcaps">lfs</span> looks into the inode map
(always at the end of the log), finds the inodes, reads the inode to find
the blocks, and pieces together the parts it wants to read.</p>
<p>When <span class="smallcaps">lfs</span> has used the entire disk up,
how does it keep appending new blocks? <span class="smallcaps">lfs</span>
divides the disk into <em>segments</em>, each consisting of a number of
blocks. Some of the blocks are still being referenced (live blocks), some
are free to be reused (free blocks). <span class="smallcaps">lfs</span>
will regularly perform garbage collection and create segments that only
contains free blocks—during garbage collection, <span class=
"smallcaps">lfs</span> copies all the live blocks in a segment to the end
of the log, then this segment becomes a free segment. Finally, when
<span class="smallcaps">lfs</span> needs to write new logs, it writes
them in free segments.</p>
<p>The challenge of garbage collection is to choose the best segment to
clean. The authors first tried to clean least utilized segment first, ie,
clean the segment with the least amount of live data. This didn’t go
well, because segments don’t get cleaned until they cross the threshold,
and a lot of segments lingers around the threshold, don’t get cleaned,
and hold up a lot of space.</p>
<p>The authors found that it’s best to categorize segments into hot and
cold segments. Hot segments are the ones that are actively updated, where
blocks are actively marked free. Cleaning hot segments isn’t very
valuable, because even if we don’t clean it, more and more of its blocks
will become free by themselves. On the other hand, cold segments are
valuable to clean, since it’s unlikely/slow to free up blocks by
itself.</p>
<p>The authors also mentioned some crash recovery and checkpoint
mechanism in the paper.</p>
<h2 id="Soft%20update" class="section">Soft update</h2>
<p><em>Soft Updates: A Solution to the Metadata Update Problem in File
Systems</em>, <span class="oldstyle-num">2000</span>.</p>
<p>In <span class="smallcaps">lfs</span> we mentioned that metadata edit
requires synchronize writes. That’s because you want to ensure the data
on disk (or any persistent storage) is always consistent. If the system
writes only a partial of the data it wishes to write, then crashed, the
disk should be in a consistent or at least recoverable state. For
example, when adding a file to a directory, adding the new inode must
happen before adding the file entry to the directory.</p>
<p>Folks has long sought to improve the performance of updating metadata,
this paper lists several existing solutions.</p>
<dl>
<dt>Nonvolatile <span class="smallcaps">ram</span> (<span class=
"smallcaps">nvram</span>)</dt>
<dd>Use <span class="smallcaps">nvram</span> to store metadata. Updating
metadata is as fast as accessing <span class="smallcaps">ram</span>, and
it persists.</dd>
<dt>Write-ahead logging</dt>
<dd>Ie, journaling. The filesystem first log the operation it’s about to
perform, and performs it. If a crash happens, the filesystem can recover
using the log.</dd>
<dt>Scheduler-enforced ordering</dt>
<dd>Modify disk request scheduler to enforce synchronous edit of
metadata. Meanwhile, the filesystem is free to edit metadata
asynchronously (since the disk request scheduler will take care of
it)</dd>
<dt>Interbuffer dependencies</dt>
<dd>Use write cache, and let the cache write-back code enforce metadata
ordering.</dd>
</dl>
<p>Soft update is similar to “interbuffer dependencies”. It maintains a
log of metadata updates, and tracks dependencies at a fine granularity
(per field or pointer), and can move the order of operations around to
avoid circular dependencies. Then it can group some updates together and
make less writes.</p>
<h2 id="Rio" class="section">Rio</h2>
<p><em>The Rio File Cache: Surviving Operating System Crashes</em>,
<span class="oldstyle-num">1996</span>.</p>
<p>The main point of Rio (<span class="smallcaps">ram/Io</span>) is to
make memory survive crashes; then the <span class="smallcaps">os</span>
doesn’t have to consistently write the cache to persistent storage.</p>
<p>Power outages can be solved by power supply with battery and dumping
memory to persistent storage when power outage occurs. Alternatively, we
can just use persistent memory. Then, during reboot, the <span class=
"smallcaps">os</span> goes through the dumped memory file to recover data
(file cache). The authors call this “warm reboot”.</p>
<p>System crash is the main challenge, because kernel crash can corrupt
the memory. The authors argue that the reason why people consider
persistent storage to be reliable and memory to be unreliable is because
of their interface: writing to disk needs drivers and explicit
procedures, etc, while writing to memory only takes a <code>mov</code>
instruction.</p>
<p>Then, protecting the file cache is just a matter of write-protecting
the memory. And there are a myriad of techniques for that already. For
example, you can use the protection that virtual memory already provides.
Just turn off the write-permission bits in the page table for file cache
pages. However, some systems allow kernel to bypass virtual memory
protection. The authors resorted to disabling processor’s ability to
bypass <span class="smallcaps">tlb</span>. This is of course
architecture-dependent.</p>
<p>Another way is to install checks for every kernel memory access, but
that’s a heavy penalty on the performance.</p>
<p>What’s more interesting is perhaps the effect of having a reliable
memory on the filesystem. First, you can turn off reliable sync writes
(this is the motivation for this paper in the first place). But also,
since memory is now permanent, metadata updates must be ordered, so that
a crash in the middle of an operation doesn’t create an inconsistent
state.</p>
<p>Nowadays, persistent memory is getting larger and cheaper to the point
that it seems possible to use it to improve <span class=
"smallcaps">io</span> performance in datacenters. Problem is, every
update has to be ordered, and you can’t control L1/2/3 cache. They can
decide to write to memory at different orders than you intended.</p>
<p>Currently there are two approaches: treat the persistent memory as a
super fast <span class="smallcaps">ssd</span>, and slap a filesystem on
it, the filesystem will take care of the dirty work. Others don’t want to
pay for the overhead of a filesystem, and want to use it as a memory. To
go this route, the programmer have to deal with the complications of
consistency/ordering.</p>
<h2 id="Scheduler%20activation" class="section">Scheduler activation</h2>
<p><em>Scheduler Activations: Effective Kernel Support for the User-level
Management of Parallelism</em>, <span class=
"oldstyle-num">1991</span>.</p>
<p>Threading can be implemented in either kernel or userspace. However,
both have their problems. If implemented in userspace, it has bad
integration with kernel schedular—userspace thread scheduler has no way
to know when a thread is going to run, and for how long. If implemented
in kernel, thread management now requires a context-switch into kernel,
which is very slow. Plus, like anthing else that goes into kernel, there
won’t be much customizability.</p>
<p>The authors present a new abstraction as the solution—scheduler
activation. The idea is to allow more cooperation between kernel and
userspace. Kernel allocates processors, and notifies the userspace when
it gives processors or takes processors away. The userspace decides what
to run on the provided processors. Finally, the userspace can request or
relinquish processors.</p>
<p>This way we get the best of both worlds: userspace thread scheduler
has more information to make decisions, meanwhile userspace can do their
own scheduling, requiring less context-switches.</p>
<p>When kernel notifies userspace of a change, it “activates” the
userspace thread scheduler (that’s where the name “scheduler activation”
comes from). A scheduler activation is like an empty kernel thread. When
kernel wants to notify userspace of something, it creates a “scheduler
activation”, assigns it a processor, and runs userspace scheduler in this
“scheduler activation”. The userspace scheduler makes decisions by the
information given in the scheduler activation by the kernel, then
proceeds to run some thread on this scheduler activation.</p>
<p>The difference between a scheduler activation and normal kernel thread
is that, when the kernel stops a scheduler activation, (maybe due to
<span class="smallcaps">i/o</span>), the kernel will create another
scheduler activation to notify the userspace that the other scheduler
activation has stopped; then the userspace scheduler can decide which
thread to run on this scheduler activation. When the original scheduler
activation is to be resumed (<span class="smallcaps">i/o</span>
completes), kernel blocks a running scheduler activation and creates a
new scheduler activation, and let userspace scheduler decide which to run
on this new scheduler activation.</p>
<p>For normal kernel threads, the kernel stops and resumes the thread
without noticing userspace, and the kernel selects what to run.</p>
<p>Critical sections (where the executing program holds some locks) is a
bit tricky in scheduler activation. When the thread is in critical
section when it is blocked or preempted, performance might take a hit (no
one else can run), or a deadlock might even appear. The solution is to
let the thread run a little bit until it exits the critical section.</p>
<p>Scheduler activation is basically the <span class=
"smallcaps">n:m</span> thread we’re taught in undergrad <span class=
"smallcaps">os</span> classes. Evidentally it isn’t very widely used,
maybe because the performance improvement isn’t worth the complexity.</p>
<h2 id="Lottery%20scheduling" class="section">Lottery scheduling</h2>
<p><em>Lottery Scheduling: Flexible Proportional-Share Resource
Management</em>, <span class="oldstyle-num">1994</span>.</p>
<p>Lottery scheduling is another probability-based algorithm, it uses a
simple algorithm to solve a otherwise difficult problem. <a id=
"footref:google-random" class="footref-anchor obviously-a-link"
aria-label="Jump to footnote" href="#footdef%3Agoogle-random">I really
like probability-based algorithms in general.<sup class=
"inline-footref">11</sup></a></p>
<div id="footdef:google-random" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Agoogle-random">11</a></div>
<div class="def-footdef">Another great example is Google’s HyperLogLog:
<a href="https://www.youtube.com/watch?v=lJYufx0bfpw"><em>A problem so
hard even Google relies on Random Chance</em></a></div>
</div>
<p>Scheduling is hard. There are so many requirements to consider:
fairness, overhead, starvation, priority, <a id=
"footref:priority-inversion" class="footref-anchor obviously-a-link"
aria-label="Jump to footnote" href=
"#footdef%3Apriority-inversion">priority inversion<sup class=
"inline-footref">12</sup></a>. However, lottery scheduling seemingly can
have its cake and eat it too, solving all of the above simultaneously
(with a catch, of course). Even better, lottery scheduling allows
flexible distribution of resources, while normal priority-based scheduler
only has corase control over processes: <a id="footref:fair-share" class=
"footref-anchor obviously-a-link" aria-label="Jump to footnote" href=
"#footdef%3Afair-share">higher priority always wins<sup class=
"inline-footref">13</sup></a>. It’s also general enough to apply to
sharing other resources, like network bandwith or memory.</p>
<div id="footdef:priority-inversion" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href=
"#footref%3Apriority-inversion">12</a></div>
<div class="def-footdef">Priority inversion is when a preempted
lower-priority process/thread holds a lock which the higher priority
process/thread needs to acquire in order to progress. In this case, the
lower-priority process is blocking the higher priority process,
effectively inverting the priority.</div>
</div>
<div id="footdef:fair-share" class="footdef">
<div class="def-footref obviously-a-link"><a aria-label=
"Jump back to main text" href="#footref%3Afair-share">13</a></div>
<div class="def-footdef">Some schduler has static priorities, some
scheduler allows dynamically adjusting priorities. And fair-share
schedulers need to monitor <span class="smallcaps">cpu</span> usage over
time and adjust priorities accordingly.</div>
</div>
<p>Here’s how it works. Suppose we have some processes and want to
allocate some proportion of execution time to each. We create
<span class="oldstyle-num">100</span> tickets, and assign each process
tickets based on their allocated proportion. Eg, if we want alloacte
<span class="oldstyle-num">30%</span> of the execution time to process A,
we assign it <span class="oldstyle-num">30</span> tickets.</p>
<p>Then, we divide time into epochs. At the start of each epoch, we
randomly draw a ticket out of the <span class="oldstyle-num">100</span>,
and run the process that owns this ticket. Over a period of time, the
total execution time of each process should match the assigned
proportion.</p>
<p>Lottery scheduling is probabilistically fair. The shorter the epoch,
and the longer the measured duration, the more accurate and fair is the
scheduling. To ensure fairness, when a process wins lottery and executes
in an epoch, only to be blocked by <span class="smallcaps">i/o</span>
midway, the scheduler would give it more ticket in the next epoch to
compensate.</p>
<p>Lottery scheduling doesn’t have starvation. As long as a process has
some ticket, the probability of it getting executed is not zero.</p>
<p>Lottery scheduling is very responsive to changes in configuration,
because any change in the allocation proportion is immediately reflected
in the next epoch. Some scheduler, like the fair-use scheduler mentioned
earlier, might take longer to adjust priorities.</p>
<p>Lottery scheduling has very low overhead. It just need to generate a
random number and find the process that owns it. It takes <span class=
"oldstyle-num">~1000</span> instructions to run scheduling; it takes
<span class="oldstyle-num">~10</span> for generating a random number, and
the rest for finding the process. The processes are stored in a linked
list, ordered by the number of tickets held.</p>
<p>Lottery scheduling handles priority inversion by allowing processes to
transfer tickets to other process. Traditional schedulers would use
priority inheritance: the higher priority process elevates the lower
priority process temporarily to execute and release the lock that the
higher priority process needs. It’s the same principle, but instead of
elevating priority, a process lends its tickets.</p>
<p>Of course, there’s always a catch. Lottery scheduling isn’t very good
at immediate, strict control over resources. Eg, in a real-time system, a
very high priority task has to be executed immediately when it comes up.
Lottery scheduling can’t run it immediately (epoch), and it can’t
guarantee to run it (randomized).</p>
<p>Also, the simple lottery scheduling can’t express response time (maybe
something needs to run immediately but won’t take a lot of <span class=
"smallcaps">cpu</span> time). We can add another parameter to represent
response time, in addition to <span class="smallcaps">cpu</span> time
allocation. Not exactly sure how that works though.</p>
<p>Nowadays, lottery scheduling isn’t used so much for <span class=
"smallcaps">cpu</span> scheduling, but widely used in networking.</p>
<h2 id="Epilogue" class="section">Epilogue</h2>
<p>That was the last classic paper. For the rest of the course, we went
through some more recent literature like Android, <span class=
"smallcaps">gfs</span>, MapReduce, Haystack. Those are no less filled
with interesting ideas, but this article is already so long and I want to
stop here.</p>
<p>Incidentally, as I’m writing this, there’s only two days left in
<span class="oldstyle-num">2023</span>. Judging from the tags, I started
this article in February <span class="oldstyle-num">15</span> this year.
Back then I didn’t know it’ll take a whole year to finish; during half
way I thought I’ll never finish this. But look where we are now!
Persistence really do get things done eventually.</p>
<p>I also started a programming project at around the same time, and that
project (after much head-scratches and typing late at night) is also
coming to fruition around this time. Looking back, I can’t believe that I
actually pulled both of these off in <span class=
"oldstyle-num">2023</span>, oh my!</p>
</article>
</main>
<footer id="postamble">
<div>
<p>Written by Yuan Fu</p>
<p>Published on 2024-01-05 Fri 19:45</p>
<p>Comment by sending a message to <a href=
"mailto:~casouri/public-inbox@lists.sr.ht?Subject=Re%3A%20Classic%20Systems%20Papers%3A%20Notes%20for%20CSE%20221">
the public inbox</a></p>
<p><a href=
"https://lists.sr.ht/~casouri/public-inbox?search=Classic%20Systems%20Papers%3A%20Notes%20for%20CSE%20221">
View existing discussions</a> | <a href=
"https://man.sr.ht/lists.sr.ht/etiquette.md">Mailing list
etiquette</a></p>
<p><a href="/note/atom.xml">RSS</a> | <a href=
"https://github.com/casouri/casouri.github.io">Source</a> | <a href=
"https://creativecommons.org/licenses/by-sa/4.0/">License</a></p>
</div>
</footer>
</body>
</html>
